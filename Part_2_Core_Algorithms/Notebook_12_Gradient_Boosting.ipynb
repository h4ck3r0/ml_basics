{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 12: Gradient Boosting\n",
    "\n",
    "Welcome to the twelfth notebook in our machine learning series. In this notebook, we will explore **Gradient Boosting**, a powerful ensemble learning technique used for both classification and regression tasks. Gradient Boosting builds on decision trees in a sequential manner, often achieving state-of-the-art results in many applications.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What is Gradient Boosting?\n",
    "- Key concepts: Boosting and Gradient Descent\n",
    "- How Gradient Boosting works\n",
    "- Implementation using scikit-learn and XGBoost\n",
    "- Advantages and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is an ensemble method that builds a strong predictive model by combining the predictions of multiple weak learners, typically decision trees. Unlike Random Forest, which builds trees independently, Gradient Boosting builds trees sequentially, with each tree correcting the errors of the previous ones.\n",
    "\n",
    "It was popularized by Jerome H. Friedman and has been extended in powerful libraries like XGBoost, LightGBM, and CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Boosting:** A technique to convert weak learners into a strong learner by focusing on the mistakes of previous models.\n",
    "- **Gradient Descent:** Gradient Boosting uses gradient descent to minimize a loss function, such as mean squared error for regression or log loss for classification.\n",
    "- **Sequential Learning:** Each new tree is trained to predict the residuals (errors) of the combined predictions of all previous trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Gradient Boosting Works\n",
    "\n",
    "1. Start with an initial prediction (often the mean of the target variable for regression).\n",
    "2. Compute the residuals (errors) between the actual values and the current predictions.\n",
    "3. Train a new decision tree to predict these residuals.\n",
    "4. Update the predictions by adding a fraction of the new tree's predictions (controlled by a learning rate).\n",
    "5. Repeat steps 2-4 for a specified number of iterations or until the residuals are minimized.\n",
    "6. The final model is the sum of all tree predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using scikit-learn and XGBoost\n",
    "\n",
    "Let's implement Gradient Boosting using both scikit-learn's `GradientBoostingClassifier` and the popular XGBoost library for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Using scikit-learn GradientBoostingClassifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f'Scikit-learn Gradient Boosting Accuracy: {accuracy_gb:.2f}')\n",
    "print('Scikit-learn Gradient Boosting Classification Report:')\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\n",
    "# 2. Using XGBoost\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'XGBoost Accuracy: {accuracy_xgb:.2f}')\n",
    "print('XGBoost Classification Report:')\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Often achieves higher accuracy than other algorithms due to its focus on correcting errors.\n",
    "- Handles various types of data and can capture complex relationships.\n",
    "- Provides feature importance for interpretability.\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally intensive and slower to train compared to Random Forest.\n",
    "- Sensitive to hyperparameters, requiring careful tuning.\n",
    "- Prone to overfitting if the number of trees or depth is too high without proper regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Gradient Boosting is a highly effective algorithm for many machine learning tasks, especially when implemented with optimized libraries like XGBoost. Its ability to iteratively improve predictions makes it a go-to choice for competitive data science and real-world applications.\n",
    "\n",
    "In the next notebook, we will explore another important topic to further enhance our machine learning skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}