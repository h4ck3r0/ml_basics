{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: Classification Made Simple\n",
    "\n",
    "Welcome to the fifth notebook in our **Machine Learning Basics for Beginners** series! After learning about linear regression for predicting continuous values, let's explore **Logistic Regression**, a supervised learning algorithm used for **classification**—predicting categories or labels instead of numbers.\n",
    "\n",
    "**What You'll Learn in This Notebook:**\n",
    "- What logistic regression is and when to use it.\n",
    "- How logistic regression works in simple terms.\n",
    "- A hands-on example of classifying emails as spam or not spam using logistic regression.\n",
    "- An interactive exercise to adjust data and see how predictions change.\n",
    "- Visualizations to understand the concept of a decision boundary.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Logistic Regression?\n",
    "\n",
    "**Logistic Regression** is a supervised learning algorithm used to predict a categorical outcome—typically whether something belongs to one of two categories (binary classification). Despite its name, it’s not about predicting numbers like linear regression; it’s about predicting probabilities that lead to a category decision.\n",
    "\n",
    "- **Goal**: Predict the probability of an outcome (e.g., the chance an email is spam) and classify it into a category (e.g., spam or not spam) based on a threshold.\n",
    "- **When to Use It**: Use logistic regression when you want to classify data into categories, especially for binary (two-class) problems. It works well when the relationship between features and the outcome can be separated by a line or curve.\n",
    "- **Examples**:\n",
    "  - Classifying emails as spam or not spam.\n",
    "  - Predicting if a patient has a disease (yes/no) based on medical data.\n",
    "  - Determining if a customer will buy a product (yes/no) based on their profile.\n",
    "\n",
    "**Analogy**: Imagine you're trying to decide if a fruit is an apple or an orange based on its color and size. Logistic regression helps draw a boundary line between \"apple\" and \"orange\" characteristics, so you can classify new fruits based on where they fall relative to that line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Does Logistic Regression Work?\n",
    "\n",
    "Logistic regression might sound complex, but let’s break it down into simple steps:\n",
    "\n",
    "1. **Start with a Linear Model**: Like linear regression, logistic regression starts by combining input features into a linear equation (something like `score = m1*x1 + m2*x2 + b`), where `x1`, `x2` are features (e.g., email length, number of suspicious words), and `m1`, `m2`, `b` are learned weights.\n",
    "2. **Convert to Probability**: Instead of predicting a number, logistic regression transforms this score into a probability (a value between 0 and 1) using a special curve called the **sigmoid function**. The sigmoid function looks like an S-shaped curve and squashes any score into a probability:\n",
    "   - If the score is very high, the probability is close to 1 (e.g., very likely spam).\n",
    "   - If the score is very low, the probability is close to 0 (e.g., very likely not spam).\n",
    "3. **Set a Threshold**: By default, if the probability is above 0.5, we classify it as one category (e.g., spam); if below 0.5, the other category (e.g., not spam). This threshold can be adjusted.\n",
    "4. **Learning**: The algorithm adjusts the weights (`m1`, `m2`, `b`) to minimize errors, making the predicted probabilities as close as possible to the actual labels in the training data.\n",
    "\n",
    "**Analogy**: Think of logistic regression as a judge in a contest. It looks at all the evidence (features), calculates a \"score\" for guilt or innocence, converts that score into a confidence level (probability), and makes a final verdict (classification) based on whether the confidence is above a certain level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Classifying Emails as Spam or Not Spam\n",
    "\n",
    "Let’s see logistic regression in action with a small dataset of emails. We’ll use two features to predict if an email is spam:\n",
    "- Number of suspicious words (like \"lottery\" or \"win\").\n",
    "- Email length (number of characters).\n",
    "\n",
    "**Dataset**:\n",
    "- Suspicious Words: 0, 2, 1, 5, 3\n",
    "- Email Length: 100, 300, 150, 400, 250\n",
    "- Is Spam (Label): No (0), Yes (1), No (0), Yes (1), Yes (1)\n",
    "\n",
    "We’ll use Python’s `scikit-learn` library to create a logistic regression model, train it on this data, and predict whether a new email is spam. Focus on the steps and output, not the code details.\n",
    "\n",
    "**Instructions**: Run the code below to see how logistic regression classifies emails and visualizes the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Our small dataset\n",
    "X = np.array([[0, 100], [2, 300], [1, 150], [5, 400], [3, 250]])  # Features: suspicious words, email length\n",
    "y = np.array([0, 1, 0, 1, 1])  # Label: 0 = Not Spam, 1 = Spam\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict for a new email with 4 suspicious words and 350 characters\n",
    "new_email = np.array([[4, 350]])\n",
    "prediction = model.predict(new_email)[0]\n",
    "probability = model.predict_proba(new_email)[0][1]  # Probability of being spam (class 1)\n",
    "print(f\"New Email (4 suspicious words, 350 chars): Predicted as {'Spam' if prediction == 1 else 'Not Spam'}\")\n",
    "print(f\"Probability of being Spam: {probability:.2%}\")\n",
    "\n",
    "# Visualize the data and decision boundary\n",
    "# Create a mesh grid for plotting decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 50, X[:, 1].max() + 50\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 10))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and points\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='Not Spam (0)', alpha=0.8)\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label='Spam (1)', alpha=0.8)\n",
    "plt.scatter(new_email[0][0], new_email[0][1], color='green', marker='x', s=200, label='New Email')\n",
    "plt.xlabel('Suspicious Words')\n",
    "plt.ylabel('Email Length (chars)')\n",
    "plt.title('Logistic Regression: Spam vs. Not Spam')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot above:\")\n",
    "print(\"- Blue dots are emails labeled as Not Spam.\")\n",
    "print(\"- Red dots are emails labeled as Spam.\")\n",
    "print(\"- The colored background shows the decision boundary: blue area predicts Not Spam, red area predicts Spam.\")\n",
    "print(\"- The green 'X' is the new email's position and predicted class based on the boundary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Exercise: Adjust Data and Classify\n",
    "\n",
    "Now it’s your turn to experiment with logistic regression! In this exercise, you can add a new email to the dataset by specifying its features and label, then see how the decision boundary changes and make a prediction for another email.\n",
    "\n",
    "**Instructions**:\n",
    "- Run the code below.\n",
    "- Enter the number of suspicious words, email length, and whether it’s spam (1 for yes, 0 for no) when prompted to add to the dataset.\n",
    "- Enter features for a new email you want to predict.\n",
    "- Observe how the boundary and prediction update with the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exercise for logistic regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Welcome to the 'Adjust Data and Classify' Exercise!\")\n",
    "print(\"You’ll add a new email to the dataset and see how the logistic regression boundary changes.\")\n",
    "\n",
    "# Original dataset\n",
    "X = np.array([[0, 100], [2, 300], [1, 150], [5, 400], [3, 250]])\n",
    "y = np.array([0, 1, 0, 1, 1])\n",
    "\n",
    "# Ask user to add a new data point\n",
    "try:\n",
    "    new_words = float(input(\"Enter number of suspicious words for new email (e.g., 3): \"))\n",
    "    new_length = float(input(\"Enter email length for new email (chars, e.g., 200): \"))\n",
    "    new_label = int(input(\"Is this email spam? (1 for Yes, 0 for No): \"))\n",
    "    if new_label not in [0, 1]:\n",
    "        raise ValueError(\"Label must be 0 or 1.\")\n",
    "    X = np.vstack([X, [new_words, new_length]])\n",
    "    y = np.append(y, new_label)\n",
    "    print(f\"Added email: {new_words} suspicious words, {new_length} chars, {'Spam' if new_label == 1 else 'Not Spam'}.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Invalid input: {e}. Using original data without changes.\")\n",
    "\n",
    "# Train the model with updated data\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Ask user for a new email to predict\n",
    "try:\n",
    "    predict_words = float(input(\"Enter suspicious words for an email to predict (e.g., 4): \"))\n",
    "    predict_length = float(input(\"Enter email length to predict (chars, e.g., 350): \"))\n",
    "    new_email = np.array([[predict_words, predict_length]])\n",
    "    prediction = model.predict(new_email)[0]\n",
    "    probability = model.predict_proba(new_email)[0][1]\n",
    "    print(f\"Predicted class for email ({predict_words} words, {predict_length} chars): {'Spam' if prediction == 1 else 'Not Spam'}\")\n",
    "    print(f\"Probability of being Spam: {probability:.2%}\")\n",
    "except ValueError:\n",
    "    new_email = np.array([[4, 350]])\n",
    "    prediction = model.predict(new_email)[0]\n",
    "    probability = model.predict_proba(new_email)[0][1]\n",
    "    print(f\"Invalid input. Defaulting to 4 words, 350 chars. Predicted: {'Spam' if prediction == 1 else 'Not Spam'}, Probability of Spam: {probability:.2%}\")\n",
    "\n",
    "# Visualize the updated data and decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 50, X[:, 1].max() + 50\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 10))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X[:-1][y[:-1] == 0][:, 0], X[:-1][y[:-1] == 0][:, 1], color='blue', label='Original Not Spam (0)', alpha=0.8)\n",
    "plt.scatter(X[:-1][y[:-1] == 1][:, 0], X[:-1][y[:-1] == 1][:, 1], color='red', label='Original Spam (1)', alpha=0.8)\n",
    "plt.scatter(X[-1, 0], X[-1, 1], color='orange', label=f\"Your Added Data ({'Spam' if y[-1] == 1 else 'Not Spam'})\", alpha=0.8)\n",
    "plt.scatter(new_email[0][0], new_email[0][1], color='green', marker='x', s=200, label='Prediction')\n",
    "plt.xlabel('Suspicious Words')\n",
    "plt.ylabel('Email Length (chars)')\n",
    "plt.title('Logistic Regression: Updated Spam vs. Not Spam')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot above:\")\n",
    "print(\"- Blue dots are original emails labeled as Not Spam.\")\n",
    "print(\"- Red dots are original emails labeled as Spam.\")\n",
    "print(\"- Orange dot is the email data you added.\")\n",
    "print(\"- The colored background shows the updated decision boundary.\")\n",
    "print(\"- The green 'X' is the predicted class for the email you chose to classify.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Considerations for Logistic Regression\n",
    "\n",
    "Logistic regression is a powerful and interpretable algorithm for classification, but it has limitations to keep in mind:\n",
    "\n",
    "- **Assumes Linear Separability**: It works best when the two classes can be separated by a straight line (or plane for multiple features). If the relationship is very complex or non-linear, other algorithms might perform better.\n",
    "- **Binary Classification by Default**: While it’s primarily for two-class problems, it can be extended to multi-class problems with techniques like \"one-vs-rest,\" though this adds complexity.\n",
    "- **Sensitive to Imbalanced Data**: If one class (e.g., spam) is much rarer than the other, logistic regression might bias toward the majority class unless adjusted.\n",
    "\n",
    "**Analogy**: Logistic regression is like drawing a straight line to separate two groups of people in a crowd based on height and weight. If the groups are mixed in a way that a straight line can’t separate them (like a circular pattern), you’d need a different method to draw the boundary.\n",
    "\n",
    "Despite these limitations, logistic regression is widely used due to its simplicity, speed, and ability to provide probabilities, making it a great starting point for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "- **Logistic Regression** is a supervised learning algorithm for classification, predicting categories (often binary, like yes/no) by estimating probabilities.\n",
    "- It works by combining features into a score, transforming it into a probability using the sigmoid function, and classifying based on a threshold (usually 0.5).\n",
    "- Use it for tasks like spam detection, disease prediction, or customer behavior classification when classes can be separated linearly.\n",
    "- Be aware of limitations: it assumes linear separability, works best for binary classification, and can struggle with imbalanced data.\n",
    "\n",
    "You’ve now learned a key classification algorithm! Logistic regression builds on the ideas of linear regression but adapts them for categorical predictions, expanding your machine learning toolkit.\n",
    "\n",
    "**What's Next?**\n",
    "Move on to **Notebook 6: Decision Trees** to learn about another powerful algorithm for both classification and regression tasks, which works by making decisions in a tree-like structure. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}