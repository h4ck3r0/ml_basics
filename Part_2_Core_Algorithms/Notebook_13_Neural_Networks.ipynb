{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 13: Neural Networks\n",
    "\n",
    "Welcome to the thirteenth notebook in our machine learning series. In this notebook, we will explore **Neural Networks**, a foundational concept in deep learning that mimics the structure and function of the human brain to solve complex problems in classification, regression, and more.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What are Neural Networks?\n",
    "- Key concepts: Neurons, Layers, and Activation Functions\n",
    "- How Neural Networks work\n",
    "- Implementation using scikit-learn and TensorFlow/Keras\n",
    "- Advantages and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Neural Networks?\n",
    "\n",
    "Neural Networks, or Artificial Neural Networks (ANNs), are a class of machine learning models inspired by the biological neural networks in the human brain. They consist of interconnected nodes (neurons) organized in layers, capable of learning patterns and relationships from data through a process called backpropagation.\n",
    "\n",
    "Neural Networks form the basis of deep learning and are particularly effective for tasks like image recognition, natural language processing, and time series prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Neurons:** The basic units of a neural network that receive input, apply a weight, add a bias, and pass the result through an activation function.\n",
    "- **Layers:** Neural Networks are organized into input, hidden, and output layers. Hidden layers process the data through multiple transformations.\n",
    "- **Activation Functions:** Functions like sigmoid, ReLU (Rectified Linear Unit), and tanh introduce non-linearity, enabling the network to solve complex problems.\n",
    "- **Weights and Biases:** Parameters learned during training to minimize the error between predictions and actual values.\n",
    "- **Backpropagation:** The algorithm used to update weights by calculating the gradient of the loss function with respect to each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Neural Networks Work\n",
    "\n",
    "1. **Forward Propagation:** Input data passes through the network layer by layer. Each neuron computes a weighted sum of inputs plus a bias, applies an activation function, and passes the output to the next layer.\n",
    "2. **Loss Calculation:** The difference between the predicted output and the actual target is computed using a loss function (e.g., mean squared error for regression, cross-entropy for classification).\n",
    "3. **Backward Propagation (Backpropagation):** The gradient of the loss with respect to each weight and bias is calculated, and weights are updated using an optimization algorithm like gradient descent.\n",
    "4. Repeat steps 1-3 for multiple epochs until the loss converges to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using scikit-learn and TensorFlow/Keras\n",
    "\n",
    "Let's implement a simple Neural Network for classification using scikit-learn's `MLPClassifier` (Multi-layer Perceptron) and a more detailed implementation with TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Using scikit-learn MLPClassifier\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f'Scikit-learn MLP Accuracy: {accuracy_mlp:.2f}')\n",
    "print('Scikit-learn MLP Classification Report:')\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "# 2. Using TensorFlow/Keras\n",
    "keras_model = Sequential([\n",
    "    Dense(100, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "keras_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "loss, accuracy_keras = keras_model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred_keras = (keras_model.predict(X_test) > 0.5).astype(int)\n",
    "print(f'Keras Neural Network Accuracy: {accuracy_keras:.2f}')\n",
    "print('Keras Neural Network Classification Report:')\n",
    "print(classification_report(y_test, y_pred_keras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Capable of learning complex, non-linear relationships in data.\n",
    "- Highly flexible and can be adapted to various tasks with different architectures.\n",
    "- Forms the foundation for advanced deep learning models.\n",
    "\n",
    "**Limitations:**\n",
    "- Requires large amounts of data to perform well.\n",
    "- Computationally expensive and often needs specialized hardware (GPUs) for training.\n",
    "- Can be difficult to interpret (black-box model) and requires careful tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Neural Networks are a cornerstone of modern machine learning and deep learning, offering powerful capabilities for solving complex problems. While they require more data and computational resources, their flexibility and performance make them invaluable for tasks ranging from image recognition to natural language processing.\n",
    "\n",
    "In the next notebook, we will explore another important technique to further expand our machine learning toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}