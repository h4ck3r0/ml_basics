{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 11: Random Forest\n",
    "\n",
    "Welcome to the eleventh notebook in our machine learning series. In this notebook, we will explore **Random Forest**, an ensemble learning method that is widely used for both classification and regression tasks. Random Forest builds on the concept of decision trees by creating a 'forest' of trees and aggregating their predictions.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What is Random Forest?\n",
    "- Key concepts: Bagging and Feature Randomness\n",
    "- How Random Forest works\n",
    "- Implementation using scikit-learn\n",
    "- Advantages and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Random Forest?\n",
    "\n",
    "Random Forest is an ensemble learning algorithm that constructs multiple decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. It was introduced by Leo Breiman and Adele Cutler.\n",
    "\n",
    "The key idea behind Random Forest is to reduce overfitting by averaging multiple trees, each trained on different subsets of the data and features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating):** Random Forest uses bagging to create multiple subsets of the training data by sampling with replacement. Each tree is trained on a different subset.\n",
    "- **Feature Randomness:** At each split in a tree, Random Forest considers only a random subset of features, which helps in making the trees less correlated.\n",
    "- **Ensemble Prediction:** For classification, the final prediction is the majority vote from all trees. For regression, it's the average of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Random Forest Works\n",
    "\n",
    "1. Create multiple subsets of the training data using bootstrap sampling.\n",
    "2. For each subset, build a decision tree, but at each split, consider only a random subset of features.\n",
    "3. Repeat until all trees are built.\n",
    "4. For a new data point, pass it through all trees and aggregate their predictions (majority vote for classification, average for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using scikit-learn\n",
    "\n",
    "Let's implement a Random Forest model using the scikit-learn library. We'll use a simple dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces overfitting by averaging multiple trees.\n",
    "- Handles large datasets with higher dimensionality well.\n",
    "- Provides feature importance, which can be useful for feature selection.\n",
    "\n",
    "**Limitations:**\n",
    "- Can be computationally expensive for very large datasets due to the number of trees.\n",
    "- Less interpretable compared to a single decision tree.\n",
    "- May require tuning of hyperparameters like the number of trees or maximum depth for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Random Forest is a powerful and versatile algorithm that is often used in practice due to its robustness and ability to handle complex datasets. By understanding its underlying concepts like bagging and feature randomness, you can effectively apply it to various machine learning problems.\n",
    "\n",
    "In the next notebook, we will explore another advanced algorithm or technique to further expand our machine learning toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
