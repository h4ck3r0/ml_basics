{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 14: Principal Component Analysis (PCA)\n",
    "\n",
    "Welcome to the fourteenth notebook in our machine learning series. In this notebook, we will explore **Principal Component Analysis (PCA)**, a widely used dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while preserving as much variance as possible.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What is PCA?\n",
    "- Key concepts: Variance, Eigenvalues, and Eigenvectors\n",
    "- How PCA works\n",
    "- Implementation using scikit-learn\n",
    "- Advantages and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a dataset while retaining most of the information (variance). It achieves this by projecting the data onto a new set of axes called principal components, which are orthogonal and ordered by the amount of variance they explain.\n",
    "\n",
    "PCA is commonly used for data visualization, noise filtering, and as a preprocessing step before applying other machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Variance:** A measure of the spread of data points. PCA seeks to maximize the variance explained by each principal component.\n",
    "- **Eigenvalues and Eigenvectors:** Eigenvectors determine the direction of the principal components, while eigenvalues indicate the amount of variance explained by each component.\n",
    "- **Principal Components:** Linear combinations of the original features that form a new coordinate system. The first principal component explains the most variance, the second explains the next most, and so on.\n",
    "- **Dimensionality Reduction:** Reducing the number of features by selecting only the top principal components that explain a significant portion of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How PCA Works\n",
    "\n",
    "1. **Standardize the Data:** Center the data by subtracting the mean and scale it by dividing by the standard deviation for each feature (ensures features with larger scales don't dominate).\n",
    "2. **Compute the Covariance Matrix:** Calculate the covariance matrix to understand how features vary together.\n",
    "3. **Eigen Decomposition:** Perform eigen decomposition on the covariance matrix to find eigenvalues and eigenvectors.\n",
    "4. **Sort Eigenvalues and Eigenvectors:** Order the eigenvectors by their corresponding eigenvalues in descending order to identify the principal components.\n",
    "5. **Select Top Components:** Choose the top k eigenvectors to form a new feature matrix (where k is the desired number of dimensions).\n",
    "6. **Transform the Data:** Project the original data onto the new feature space using the selected eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using scikit-learn\n",
    "\n",
    "Let's implement PCA using scikit-learn to reduce the dimensionality of a dataset and visualize the results. We'll also use it as a preprocessing step for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset with high dimensionality\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Visualize the reduced data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', label='Train')\n",
    "plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='viridis', marker='x', label='Test')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('PCA: Data Reduced to 2 Dimensions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance ratio\n",
    "print(f'Explained Variance Ratio for 2 components: {pca.explained_variance_ratio_}')\n",
    "print(f'Total Explained Variance Ratio: {sum(pca.explained_variance_ratio_):.2f}')\n",
    "\n",
    "# Use PCA as preprocessing for classification (reduce to 5 dimensions)\n",
    "pca_classifier = PCA(n_components=5)\n",
    "X_train_pca_classifier = pca_classifier.fit_transform(X_train_scaled)\n",
    "X_test_pca_classifier = pca_classifier.transform(X_test_scaled)\n",
    "\n",
    "# Train a Random Forest Classifier on reduced data\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_pca_classifier, y_train)\n",
    "y_pred = rf.predict(X_test_pca_classifier)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Random Forest after PCA (5 components): {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Reduces computational complexity by lowering the number of features.\n",
    "- Helps in visualizing high-dimensional data by reducing it to 2 or 3 dimensions.\n",
    "- Can improve model performance by removing noise and redundant features.\n",
    "\n",
    "**Limitations:**\n",
    "- Assumes linear relationships between variables, which may not capture complex patterns.\n",
    "- Loss of interpretability since principal components are combinations of original features.\n",
    "- Requires standardization of data to ensure fair contribution from all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Principal Component Analysis is a powerful tool for dimensionality reduction, making it easier to visualize and process high-dimensional data. While it has limitations in capturing non-linear relationships, it remains a fundamental technique in the machine learning preprocessing pipeline.\n",
    "\n",
    "In the next notebook, we will explore another important topic to further enhance our machine learning skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}