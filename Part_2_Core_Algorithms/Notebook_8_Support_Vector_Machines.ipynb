{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines: Finding the Best Boundary\n",
    "\n",
    "Welcome to the eighth notebook in our **Machine Learning Basics for Beginners** series! After exploring K-Nearest Neighbors, let's dive into **Support Vector Machines (SVMs)**, a powerful supervised learning algorithm primarily used for classification, though it can also handle regression. SVMs are great at finding the best boundary to separate different classes.\n",
    "\n",
    "**What You'll Learn in This Notebook:**\n",
    "- What Support Vector Machines are and when to use them.\n",
    "- How SVMs work in simple terms.\n",
    "- A hands-on example of classifying data points into two classes using SVM.\n",
    "- An interactive exercise to adjust data and see how the decision boundary changes.\n",
    "- Visualizations to understand the concept of margins and support vectors.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are Support Vector Machines?\n",
    "\n",
    "**Support Vector Machines (SVMs)** are a supervised learning algorithm designed to classify data by finding the best possible boundary (or hyperplane) that separates different classes with the widest possible margin. While SVMs are mainly used for classification, they can also be adapted for regression tasks.\n",
    "\n",
    "- **Goal**: Find a decision boundary that maximizes the margin (distance) between the closest points of different classes, ensuring the best separation.\n",
    "- **When to Use It**: Use SVMs for binary classification tasks (e.g., spam vs. not spam) when you want a model that can handle both linearly separable data and, with some tricks (like kernels), non-linearly separable data. They work well with small to medium-sized datasets.\n",
    "- **Examples**:\n",
    "  - Classifying emails as spam or not spam based on features like word frequency.\n",
    "  - Identifying whether a tumor is benign or malignant based on medical measurements.\n",
    "  - Separating images of cats and dogs based on pixel features.\n",
    "\n",
    "**Analogy**: Imagine you’re trying to separate two groups of toys on a table with a straight line drawn by a ruler. You want the line to be positioned so that the toys from each group are as far away from it as possible, ensuring no toy is too close to the wrong side. SVMs do this by finding the optimal dividing line (or boundary) with the largest gap between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Do Support Vector Machines Work?\n",
    "\n",
    "SVMs might sound complex, but the core idea is simple: find the best boundary to separate classes. Let’s break it down step by step:\n",
    "\n",
    "1. **Find the Hyperplane**: In a 2D space, a hyperplane is just a line that separates two classes. In higher dimensions, it becomes a plane or more complex surface. SVM looks for the hyperplane that best divides the data points of different classes.\n",
    "2. **Maximize the Margin**: Among all possible hyperplanes that separate the classes, SVM chooses the one with the largest margin—the distance between the hyperplane and the closest data points (called support vectors) from each class. A larger margin means better separation and often better generalization to new data.\n",
    "3. **Support Vectors**: These are the data points closest to the hyperplane. They are critical because they define the position and orientation of the hyperplane. If these points move, the hyperplane might change.\n",
    "4. **Handle Non-Linear Data (with Kernels)**: If the data isn’t linearly separable (can’t be split by a straight line), SVMs use a \"kernel trick\" to transform the data into a higher-dimensional space where a linear boundary can work. Common kernels include polynomial and radial basis function (RBF).\n",
    "5. **Prediction**: For a new data point, SVM determines which side of the hyperplane it falls on to classify it into one of the classes.\n",
    "\n",
    "**Analogy**: Think of SVM as a referee drawing a line between two teams on a field. The referee wants the line to be as far as possible from the nearest players on both sides (maximizing the margin) so there’s no confusion about which side a player is on. The nearest players (support vectors) are the ones the referee watches closely to set the line.\n",
    "\n",
    "**Key Advantage**: SVMs are effective at finding clear boundaries even in complex data (with kernels) and are less prone to overfitting when the margin is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Classifying Points with SVM\n",
    "\n",
    "Let’s see SVM in action with a small synthetic dataset of points in 2D space, representing two classes (e.g., two types of fruits based on size and color intensity).\n",
    "\n",
    "**Dataset** (simplified):\n",
    "- Feature 1 (e.g., Size): 1, 2, 5, 6, 1.5\n",
    "- Feature 2 (e.g., Color Intensity): 1, 1.5, 4, 5, 2\n",
    "- Class (Label): 0, 0, 1, 1, 0\n",
    "\n",
    "We’ll use Python’s `scikit-learn` library to create an SVM model, train it on this data, and predict the class of a new point. Focus on the steps and output, not the code details.\n",
    "\n",
    "**Instructions**: Run the code below to see how SVM classifies points and visualizes the decision boundary and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Our small dataset\n",
    "X = np.array([[1, 1], [2, 1.5], [5, 4], [6, 5], [1.5, 2]])  # Features: size, color intensity\n",
    "y = np.array([0, 0, 1, 1, 0])  # Labels: class 0 or 1\n",
    "\n",
    "# Create and train the SVM model with a linear kernel\n",
    "model = SVC(kernel='linear', C=1.0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict for a new point with size=3, color intensity=2.5\n",
    "new_point = np.array([[3, 2.5]])\n",
    "prediction = model.predict(new_point)[0]\n",
    "print(f\"New Point (size=3, color intensity=2.5): Predicted as Class {prediction}\")\n",
    "\n",
    "# Get support vectors\n",
    "support_vectors = model.support_vectors_\n",
    "print(f\"Support Vectors (points defining the boundary):\\n{support_vectors}\")\n",
    "\n",
    "# Visualize the data, decision boundary, and support vectors\n",
    "# Create a mesh grid for decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='blue', label='Class 0', alpha=0.8)\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='red', label='Class 1', alpha=0.8)\n",
    "plt.scatter(new_point[0][0], new_point[0][1], color='green', marker='x', s=200, label='New Point')\n",
    "# Highlight support vectors\n",
    "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], color='yellow', edgecolor='black', s=150, alpha=0.5, label='Support Vectors')\n",
    "# Plot the decision boundary line\n",
    "w = model.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx_line = np.linspace(x_min, x_max)\n",
    "yy_line = a * xx_line - (model.intercept_[0]) / w[1]\n",
    "plt.plot(xx_line, yy_line, 'k-', label='Decision Boundary')\n",
    "# Plot margins (approximate)\n",
    "margin = 1 / np.sqrt(np.sum(model.coef_ ** 2))\n",
    "yy_down = yy_line - np.sqrt(1 + a ** 2) * margin\n",
    "yy_up = yy_line + np.sqrt(1 + a ** 2) * margin\n",
    "plt.plot(xx_line, yy_down, 'k--')\n",
    "plt.plot(xx_line, yy_up, 'k--')\n",
    "plt.xlabel('Feature 1 (e.g., Size)')\n",
    "plt.ylabel('Feature 2 (e.g., Color Intensity)')\n",
    "plt.title('Support Vector Machine: Decision Boundary and Margins')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot above:\")\n",
    "print(\"- Blue dots are Class 0 points.\")\n",
    "print(\"- Red dots are Class 1 points.\")\n",
    "print(\"- The colored background shows the decision regions.\")\n",
    "print(\"- The black solid line is the decision boundary (hyperplane).\")\n",
    "print(\"- The black dashed lines show the margins (maximum separation).\")\n",
    "print(\"- Yellow highlighted points are the support vectors defining the boundary.\")\n",
    "print(\"- The green 'X' is the new point being classified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Exercise: Adjust Data and See the Boundary\n",
    "\n",
    "Now it’s your turn to experiment with SVM! In this exercise, you can add a new data point to the dataset by specifying its features and class, then see how the decision boundary and margins change. You’ll also choose a new point to classify.\n",
    "\n",
    "**Instructions**:\n",
    "- Run the code below.\n",
    "- Enter values for Feature 1 (e.g., Size), Feature 2 (e.g., Color Intensity), and Class (0 or 1) to add to the dataset.\n",
    "- Specify a new point to predict by entering its features.\n",
    "- Observe how the boundary, margins, and support vectors update with the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exercise for SVM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"Welcome to the 'Adjust Data and See the Boundary' Exercise!\")\n",
    "print(\"You’ll add a new point to the dataset and see how the SVM decision boundary changes.\")\n",
    "\n",
    "# Original dataset\n",
    "X = np.array([[1, 1], [2, 1.5], [5, 4], [6, 5], [1.5, 2]])\n",
    "y = np.array([0, 0, 1, 1, 0])\n",
    "\n",
    "# Ask user to add a new data point\n",
    "try:\n",
    "    new_f1 = float(input(\"Enter Feature 1 for new point (e.g., Size, like 3.5): \"))\n",
    "    new_f2 = float(input(\"Enter Feature 2 for new point (e.g., Color Intensity, like 3.0): \"))\n",
    "    new_class = int(input(\"Enter Class for new point (0 or 1): \"))\n",
    "    if new_class not in [0, 1]:\n",
    "        raise ValueError(\"Class must be 0 or 1.\")\n",
    "    X = np.vstack([X, [new_f1, new_f2]])\n",
    "    y = np.append(y, new_class)\n",
    "    print(f\"Added point: Feature 1={new_f1}, Feature 2={new_f2}, Class={new_class}.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Invalid input: {e}. Using original data without changes.\")\n",
    "\n",
    "# Train the model with updated data\n",
    "model = SVC(kernel='linear', C=1.0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Ask user for a new point to predict\n",
    "try:\n",
    "    predict_f1 = float(input(\"Enter Feature 1 to predict class (e.g., Size, like 3): \"))\n",
    "    predict_f2 = float(input(\"Enter Feature 2 to predict class (e.g., Color Intensity, like 2.5): \"))\n",
    "    new_point = np.array([[predict_f1, predict_f2]])\n",
    "    prediction = model.predict(new_point)[0]\n",
    "    print(f\"Predicted class for point (Feature 1={predict_f1}, Feature 2={predict_f2}): Class {prediction}\")\n",
    "except ValueError:\n",
    "    new_point = np.array([[3, 2.5]])\n",
    "    prediction = model.predict(new_point)[0]\n",
    "    print(f\"Invalid input. Defaulting to Feature 1=3, Feature 2=2.5. Predicted class: {prediction}\")\n",
    "\n",
    "# Get support vectors\n",
    "support_vectors = model.support_vectors_\n",
    "print(f\"Updated Support Vectors (points defining the boundary):\\n{support_vectors}\")\n",
    "\n",
    "# Visualize the updated data, decision boundary, and support vectors\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X[:-1][y[:-1] == 0][:, 0], X[:-1][y[:-1] == 0][:, 1], color='blue', label='Original Class 0', alpha=0.8)\n",
    "plt.scatter(X[:-1][y[:-1] == 1][:, 0], X[:-1][y[:-1] == 1][:, 1], color='red', label='Original Class 1', alpha=0.8)\n",
    "plt.scatter(X[-1, 0], X[-1, 1], color='orange', label=f\"Your Added Data (Class {y[-1]})\", alpha=0.8)\n",
    "plt.scatter(new_point[0][0], new_point[0][1], color='green', marker='x', s=200, label='Prediction')\n",
    "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], color='yellow', edgecolor='black', s=150, alpha=0.5, label='Support Vectors')\n",
    "w = model.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx_line = np.linspace(x_min, x_max)\n",
    "yy_line = a * xx_line - (model.intercept_[0]) / w[1]\n",
    "plt.plot(xx_line, yy_line, 'k-', label='Decision Boundary')\n",
    "margin = 1 / np.sqrt(np.sum(model.coef_ ** 2))\n",
    "yy_down = yy_line - np.sqrt(1 + a ** 2) * margin\n",
    "yy_up = yy_line + np.sqrt(1 + a ** 2) * margin\n",
    "plt.plot(xx_line, yy_down, 'k--')\n",
    "plt.plot(xx_line, yy_up, 'k--')\n",
    "plt.xlabel('Feature 1 (e.g., Size)')\n",
    "plt.ylabel('Feature 2 (e.g., Color Intensity)')\n",
    "plt.title('Support Vector Machine: Updated Decision Boundary')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot above:\")\n",
    "print(\"- Blue dots are original Class 0 points.\")\n",
    "print(\"- Red dots are original Class 1 points.\")\n",
    "print(\"- Orange dot is the point you added.\")\n",
    "print(\"- The colored background shows the updated decision regions.\")\n",
    "print(\"- The black solid line is the updated decision boundary.\")\n",
    "print(\"- The black dashed lines show the updated margins.\")\n",
    "print(\"- Yellow highlighted points are the updated support vectors.\")\n",
    "print(\"- The green 'X' is the new point being classified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Considerations for Support Vector Machines\n",
    "\n",
    "SVMs are powerful for classification, especially when data is well-structured, but they come with some considerations to keep in mind:\n",
    "\n",
    "- **Choosing the Kernel**: For non-linearly separable data, selecting the right kernel (e.g., RBF, polynomial) and tuning its parameters is crucial. The wrong kernel can lead to poor performance.\n",
    "- **Computationally Intensive**: SVMs can be slow to train on large datasets because they need to calculate distances and optimize the margin, especially with complex kernels.\n",
    "- **Sensitive to Feature Scaling**: Since SVMs rely on distances between points, features on different scales (e.g., one in meters, another in kilometers) can distort the margin. Features should be normalized or standardized.\n",
    "- **Parameter Tuning (C)**: The parameter `C` controls the trade-off between maximizing the margin and allowing misclassifications. A high `C` tries to classify all points correctly (risking overfitting), while a low `C` prioritizes a wider margin (risking underfitting).\n",
    "\n",
    "**Analogy**: SVM is like drawing a fence between two yards to keep dogs and cats separate. If the yards are oddly shaped (non-linear data), you need a special fence design (kernel). If you obsess over every pet staying on their side (high C), the fence might be too tight and impractical. If you’re too lax (low C), the fence might not separate them well.\n",
    "\n",
    "Despite these challenges, SVMs are highly effective for many classification tasks, especially when combined with the right kernel and tuning, and they provide a clear geometric interpretation of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "- **Support Vector Machines (SVMs)** are a supervised learning algorithm for classification (and regression), finding the best hyperplane to separate classes with the widest margin.\n",
    "- They work by maximizing the margin between classes, relying on support vectors (closest points) to define the boundary, and can handle non-linear data with kernels.\n",
    "- Use them for binary classification tasks like spam detection or medical diagnosis when you want a strong boundary and have small to medium-sized datasets.\n",
    "- Be aware of limitations: they require careful kernel selection and parameter tuning, are slow on large data, and need scaled features for accurate results.\n",
    "\n",
    "You’ve now learned a robust boundary-based algorithm! SVMs introduce the concept of margins and optimal separation, which is a powerful approach for classification problems.\n",
    "\n",
    "**What's Next?**\n",
    "Move on to **Notebook 9: K-Means Clustering** to learn about an unsupervised learning algorithm for grouping data without labels. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}