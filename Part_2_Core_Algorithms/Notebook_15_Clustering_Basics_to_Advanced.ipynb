{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 15: Clustering - Basics to Advanced\n",
    "\n",
    "Welcome to the fifteenth notebook in our machine learning series under **Part_2_Core_Algorithms**. In this notebook, we will explore **Clustering**, a fundamental unsupervised learning technique used to group similar data points based on their features. This notebook builds on and expands the concepts introduced in Notebook 9 (K-Means Clustering) to provide a comprehensive understanding from basics to advanced methods.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What is Clustering?\n",
    "- Key Concepts: Distance Metrics, Centroids, Hierarchical vs. Partitional Clustering\n",
    "- Types of Clustering Algorithms\n",
    "- How Clustering Works\n",
    "- Implementation Using scikit-learn\n",
    "- Evaluating Clustering Results\n",
    "- Advantages and Limitations\n",
    "- Conclusion and Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Clustering?\n",
    "\n",
    "Clustering is an unsupervised machine learning technique that aims to partition a dataset into groups (or clusters) such that data points within the same cluster are more similar to each other than to those in other clusters. Unlike supervised learning, clustering does not rely on labeled data; instead, it discovers inherent structures or patterns in the data.\n",
    "\n",
    "Clustering is widely used in applications such as customer segmentation, image compression, anomaly detection, and bioinformatics (e.g., grouping genes with similar expression patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Distance Metrics:** Measures used to quantify the similarity or dissimilarity between data points. Common metrics include:\n",
    "  - Euclidean Distance (straight-line distance between points)\n",
    "  - Manhattan Distance (sum of absolute differences)\n",
    "  - Cosine Similarity (measures angle between vectors, often used for text data)\n",
    "- **Centroids:** Representative points (often the mean) of a cluster, used in algorithms like K-Means to define the center of a cluster.\n",
    "- **Hierarchical vs. Partitional Clustering:**\n",
    "  - **Hierarchical:** Builds a hierarchy of clusters, either bottom-up (agglomerative) or top-down (divisive).\n",
    "  - **Partitional:** Divides data into a predefined number of clusters (e.g., K-Means).\n",
    "- **Cluster Assignment:** The process of assigning data points to clusters based on proximity or similarity to cluster representatives.\n",
    "- **Objective Function:** Many clustering algorithms optimize a criterion, such as minimizing within-cluster variance or maximizing between-cluster separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Clustering Algorithms\n",
    "\n",
    "Clustering algorithms can be broadly categorized based on their approach to grouping data. Below are some of the most common and important methods:\n",
    "\n",
    "- **K-Means Clustering:** A partitional method that assigns data points to K clusters by minimizing the variance within clusters. It iteratively updates centroids and reassigns points.\n",
    "- **Hierarchical Clustering:** Builds a dendrogram (tree-like structure) of clusters. It can be:\n",
    "  - **Agglomerative:** Starts with each point as a cluster and merges the closest pairs iteratively.\n",
    "  - **Divisive:** Starts with all points in one cluster and splits them iteratively.\n",
    "- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** Groups points based on density, identifying clusters as dense regions separated by sparse areas. It can detect arbitrarily shaped clusters and mark outliers as noise.\n",
    "- **Gaussian Mixture Models (GMM):** A probabilistic model that assumes data points are generated from a mixture of Gaussian distributions. It assigns probabilities of belonging to clusters (soft clustering).\n",
    "- **Spectral Clustering:** Uses graph theory and eigenvalues of a similarity matrix to cluster data, effective for non-linearly separable clusters.\n",
    "- **Mean Shift:** A mode-seeking algorithm that identifies clusters by finding dense regions in the data space, shifting points towards the mode (peak) of the density.\n",
    "- **Affinity Propagation:** Identifies exemplars (representative points) through message passing between data points, determining clusters based on similarity without needing a predefined number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Clustering Works\n",
    "\n",
    "While specific algorithms differ, the general process of clustering involves the following steps:\n",
    "\n",
    "1. **Initialization:** Choose initial cluster representatives (e.g., random centroids in K-Means, or each point as a cluster in hierarchical clustering).\n",
    "2. **Assignment:** Assign data points to the nearest or most similar cluster based on a distance or similarity metric.\n",
    "3. **Update:** Recalculate cluster representatives (e.g., update centroids as the mean of assigned points in K-Means, or merge clusters in hierarchical methods).\n",
    "4. **Iteration:** Repeat steps 2 and 3 until convergence (e.g., centroids stabilize, or a stopping criterion like a set number of clusters is reached).\n",
    "5. **Output:** Return the final set of clusters and their assignments.\n",
    "\n",
    "Each algorithm has unique mechanisms (e.g., density estimation in DBSCAN, probabilistic assignment in GMM), which we will explore through implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using scikit-learn\n",
    "\n",
    "Let's implement several clustering algorithms using scikit-learn on both synthetic datasets and a real-world dataset to demonstrate how they group data points. We'll visualize the results to compare their behaviors and characteristics. We'll start with synthetic datasets to understand the algorithms' behaviors and then apply a few algorithms to the well-known Iris dataset for a practical example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles, load_iris\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, MeanShift, AffinityPropagation\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Function to plot clustering results\n",
    "def plot_clusters(X, labels, title, centers=None, feature_indices=(0, 1)):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, feature_indices[0]], X[:, feature_indices[1]], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "    if centers is not None:\n",
    "        plt.scatter(centers[:, feature_indices[0]], centers[:, feature_indices[1]], c='red', s=200, alpha=0.75, marker='X', label='Centroids')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Generate synthetic datasets for different clustering scenarios\n",
    "# 1. Simple blobs (well-separated clusters for K-Means, GMM)\n",
    "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "# 2. Moon shapes (non-linearly separable, for Spectral, DBSCAN)\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "# 3. Concentric circles (for Spectral, DBSCAN)\n",
    "X_circles, y_circles = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
    "\n",
    "# 1. K-Means Clustering on Blobs\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_blobs)\n",
    "plot_clusters(X_blobs, kmeans_labels, 'K-Means Clustering on Blobs', centers=kmeans.cluster_centers_)\n",
    "silhouette_kmeans = silhouette_score(X_blobs, kmeans_labels)\n",
    "print(f'K-Means Silhouette Score: {silhouette_kmeans:.2f}')\n",
    "\n",
    "# 2. Gaussian Mixture Model (GMM) on Blobs\n",
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(X_blobs)\n",
    "plot_clusters(X_blobs, gmm_labels, 'Gaussian Mixture Model on Blobs', centers=gmm.means_)\n",
    "silhouette_gmm = silhouette_score(X_blobs, gmm_labels)\n",
    "print(f'GMM Silhouette Score: {silhouette_gmm:.2f}')\n",
    "\n",
    "# 3. Hierarchical Clustering (Agglomerative) on Blobs\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4)\n",
    "hierarchical_labels = hierarchical.fit_predict(X_blobs)\n",
    "plot_clusters(X_blobs, hierarchical_labels, 'Hierarchical Clustering on Blobs')\n",
    "silhouette_hierarchical = silhouette_score(X_blobs, hierarchical_labels)\n",
    "print(f'Hierarchical Silhouette Score: {silhouette_hierarchical:.2f}')\n",
    "\n",
    "# 4. DBSCAN on Moon Shapes\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_moons)\n",
    "plot_clusters(X_moons, dbscan_labels, 'DBSCAN on Moon Shapes')\n",
    "if len(np.unique(dbscan_labels)) > 1:  # Silhouette score requires at least 2 clusters\n",
    "    silhouette_dbscan = silhouette_score(X_moons, dbscan_labels)\n",
    "    print(f'DBSCAN Silhouette Score: {silhouette_dbscan:.2f}')\n",
    "else:\n",
    "    print('DBSCAN Silhouette Score: Not applicable (single cluster or noise only)')\n",
    "\n",
    "# 5. Spectral Clustering on Concentric Circles\n",
    "spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=42)\n",
    "spectral_labels = spectral.fit_predict(X_circles)\n",
    "plot_clusters(X_circles, spectral_labels, 'Spectral Clustering on Concentric Circles')\n",
    "silhouette_spectral = silhouette_score(X_circles, spectral_labels)\n",
    "print(f'Spectral Clustering Silhouette Score: {silhouette_spectral:.2f}')\n",
    "\n",
    "# 6. Mean Shift on Blobs (bandwidth can be tuned or estimated)\n",
    "mean_shift = MeanShift()\n",
    "mean_shift_labels = mean_shift.fit_predict(X_blobs)\n",
    "plot_clusters(X_blobs, mean_shift_labels, 'Mean Shift on Blobs', centers=mean_shift.cluster_centers_)\n",
    "if len(np.unique(mean_shift_labels)) > 1:\n",
    "    silhouette_mean_shift = silhouette_score(X_blobs, mean_shift_labels)\n",
    "    print(f'Mean Shift Silhouette Score: {silhouette_mean_shift:.2f}')\n",
    "else:\n",
    "    print('Mean Shift Silhouette Score: Not applicable (single cluster)')\n",
    "\n",
    "# 7. Affinity Propagation on Blobs (no need to specify number of clusters)\n",
    "affinity = AffinityPropagation(random_state=42)\n",
    "affinity_labels = affinity.fit_predict(X_blobs)\n",
    "plot_clusters(X_blobs, affinity_labels, 'Affinity Propagation on Blobs', centers=X_blobs[affinity.cluster_centers_indices_])\n",
    "if len(np.unique(affinity_labels)) > 1:\n",
    "    silhouette_affinity = silhouette_score(X_blobs, affinity_labels)\n",
    "    print(f'Affinity Propagation Silhouette Score: {silhouette_affinity:.2f}')\n",
    "else:\n",
    "    print('Affinity Propagation Silhouette Score: Not applicable (single cluster)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering on a Real-World Dataset: Iris Dataset\n",
    "\n",
    "Now, let's apply a few clustering algorithms to the Iris dataset, a well-known dataset in machine learning. The Iris dataset contains measurements of sepal and petal dimensions for three species of iris flowers. Although it is a labeled dataset, we will ignore the labels and use clustering to see if the algorithms can identify natural groupings that correspond to the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target  # We'll use this only for reference, not for clustering\n",
    "\n",
    "# Standardize the features since clustering algorithms are sensitive to scale\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# 1. K-Means Clustering on Iris Dataset\n",
    "kmeans_iris = KMeans(n_clusters=3, random_state=42)  # We know there are 3 species\n",
    "kmeans_iris_labels = kmeans_iris.fit_predict(X_iris_scaled)\n",
    "plot_clusters(X_iris_scaled, kmeans_iris_labels, 'K-Means Clustering on Iris Dataset (Sepal Length vs Width)', centers=kmeans_iris.cluster_centers_, feature_indices=(0, 1))\n",
    "plot_clusters(X_iris_scaled, kmeans_iris_labels, 'K-Means Clustering on Iris Dataset (Petal Length vs Width)', centers=kmeans_iris.cluster_centers_, feature_indices=(2, 3))\n",
    "silhouette_kmeans_iris = silhouette_score(X_iris_scaled, kmeans_iris_labels)\n",
    "print(f'K-Means Silhouette Score on Iris: {silhouette_kmeans_iris:.2f}')\n",
    "\n",
    "# 2. Gaussian Mixture Model (GMM) on Iris Dataset\n",
    "gmm_iris = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm_iris_labels = gmm_iris.fit_predict(X_iris_scaled)\n",
    "plot_clusters(X_iris_scaled, gmm_iris_labels, 'Gaussian Mixture Model on Iris Dataset (Sepal Length vs Width)', centers=gmm_iris.means_, feature_indices=(0, 1))\n",
    "plot_clusters(X_iris_scaled, gmm_iris_labels, 'Gaussian Mixture Model on Iris Dataset (Petal Length vs Width)', centers=gmm_iris.means_, feature_indices=(2, 3))\n",
    "silhouette_gmm_iris = silhouette_score(X_iris_scaled, gmm_iris_labels)\n",
    "print(f'GMM Silhouette Score on Iris: {silhouette_gmm_iris:.2f}')\n",
    "\n",
    "# 3. DBSCAN on Iris Dataset\n",
    "dbscan_iris = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_iris_labels = dbscan_iris.fit_predict(X_iris_scaled)\n",
    "plot_clusters(X_iris_scaled, dbscan_iris_labels, 'DBSCAN on Iris Dataset (Sepal Length vs Width)', feature_indices=(0, 1))\n",
    "plot_clusters(X_iris_scaled, dbscan_iris_labels, 'DBSCAN on Iris Dataset (Petal Length vs Width)', feature_indices=(2, 3))\n",
    "if len(np.unique(dbscan_iris_labels)) > 1:\n",
    "    silhouette_dbscan_iris = silhouette_score(X_iris_scaled, dbscan_iris_labels)\n",
    "    print(f'DBSCAN Silhouette Score on Iris: {silhouette_dbscan_iris:.2f}')\n",
    "else:\n",
    "    print('DBSCAN Silhouette Score on Iris: Not applicable (single cluster or noise only)')\n",
    "\n",
    "# For reference, plot the true labels\n",
    "plot_clusters(X_iris_scaled, y_iris, 'True Iris Species (Sepal Length vs Width)', feature_indices=(0, 1))\n",
    "plot_clusters(X_iris_scaled, y_iris, 'True Iris Species (Petal Length vs Width)', feature_indices=(2, 3))\n",
    "print('Note: The true species labels are shown for comparison, but were not used in clustering.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Clustering Results\n",
    "\n",
    "Since clustering is unsupervised, evaluating the quality of clusters can be challenging without ground truth labels. Several metrics and methods help assess clustering performance:\n",
    "\n",
    "- **Silhouette Score:** Measures how similar a point is to its own cluster compared to other clusters. Ranges from -1 to 1, with higher values indicating better-defined clusters.\n",
    "- **Elbow Method:** Used in K-Means to plot the within-cluster sum of squares (inertia) against the number of clusters (K). The \"elbow\" point suggests an optimal K where adding more clusters yields diminishing returns.\n",
    "- **Davies-Bouldin Index:** Measures the average similarity between each cluster and its most similar cluster, where lower values indicate better clustering.\n",
    "- **Visual Inspection:** Plotting the data with cluster labels (especially in 2D or 3D) to visually assess if clusters make intuitive sense.\n",
    "\n",
    "In the code above, we computed Silhouette Scores for each algorithm to provide a quantitative measure of clustering quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages of Clustering:**\n",
    "- Discovers hidden patterns or structures in unlabeled data, useful for exploratory data analysis.\n",
    "- Applicable to a wide range of domains, from market segmentation to image processing.\n",
    "- Algorithms like DBSCAN and Spectral Clustering can handle non-linearly separable or arbitrarily shaped clusters.\n",
    "- Provides insights without requiring prior knowledge of class labels.\n",
    "\n",
    "**Limitations of Clustering:**\n",
    "- Sensitive to the choice of parameters (e.g., number of clusters in K-Means, epsilon in DBSCAN), which often require trial and error or domain knowledge.\n",
    "- Performance depends on the distance metric used; inappropriate metrics can lead to poor clustering.\n",
    "- Struggles with high-dimensional data due to the curse of dimensionality, often requiring dimensionality reduction (e.g., PCA) first.\n",
    "- Some algorithms (e.g., K-Means) assume spherical or evenly sized clusters, failing on complex shapes or varying densities.\n",
    "- Scalability issues with large datasets for algorithms like Hierarchical Clustering or Affinity Propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Practical Applications\n",
    "\n",
    "Clustering is a versatile and powerful technique in unsupervised learning, enabling the discovery of natural groupings in data without predefined labels. From the simplicity of K-Means to the flexibility of DBSCAN and the sophistication of Spectral Clustering, each algorithm offers unique strengths suited to different data characteristics and problem domains.\n",
    "\n",
    "**Practical Applications:**\n",
    "- **Customer Segmentation:** Grouping customers by purchasing behavior for targeted marketing.\n",
    "- **Image Segmentation:** Partitioning images into regions for object detection or medical imaging.\n",
    "- **Anomaly Detection:** Identifying outliers as small or isolated clusters (e.g., fraud detection).\n",
    "- **Recommendation Systems:** Clustering users or items to suggest similar content.\n",
    "- **Genomics:** Grouping genes or proteins with similar functions or expression patterns.\n",
    "\n",
    "Understanding the basics and nuances of various clustering algorithms equips you to choose the right method for your data and task. In future notebooks, we may explore advanced applications or hybrid approaches combining clustering with other machine learning techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}