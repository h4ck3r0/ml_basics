{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors: Classification and Regression by Similarity\n",
    "\n",
    "Welcome to the seventh notebook in our **Machine Learning Basics for Beginners** series! After exploring decision trees, let's dive into **K-Nearest Neighbors (KNN)**, a simple yet powerful supervised learning algorithm used for both classification and regression. KNN makes predictions based on the idea of similarity—finding the closest examples in the data.\n",
    "\n",
    "**What You'll Learn in This Notebook:**\n",
    "- What K-Nearest Neighbors is and when to use it.\n",
    "- How KNN works in simple terms.\n",
    "- A hands-on example of classifying flowers based on measurements using KNN.\n",
    "- An interactive exercise to adjust the number of neighbors and see how predictions change.\n",
    "- Visualizations to understand the concept of nearest neighbors.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is K-Nearest Neighbors?\n",
    "\n",
    "**K-Nearest Neighbors (KNN)** is a supervised learning algorithm that makes predictions based on the similarity of data points. It’s often called a \"lazy learning\" algorithm because it doesn’t build a model during training—it simply stores the data and makes decisions at prediction time.\n",
    "\n",
    "- **Goal**: For a new data point, find the `K` closest points (neighbors) in the training data and use their labels or values to make a prediction.\n",
    "- **When to Use It**: Use KNN for classification (e.g., identifying categories like spam or not spam) or regression (e.g., predicting a numerical value like house price) when you have labeled data and believe similar inputs should have similar outputs. It works well with small to medium-sized datasets.\n",
    "- **Examples**:\n",
    "  - Classifying a flower as a specific species based on petal and sepal measurements.\n",
    "  - Predicting a house price based on the prices of nearby houses with similar features.\n",
    "  - Recommending movies by finding users with similar tastes.\n",
    "\n",
    "**Analogy**: Imagine you’re trying to decide if a new friend likes action movies. You look at your 5 closest friends (based on age or other traits). If most of them like action movies, you guess your new friend will too. KNN works the same way—guessing based on the \"nearest\" examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Does K-Nearest Neighbors Work?\n",
    "\n",
    "KNN is straightforward and intuitive. Here’s how it works step by step:\n",
    "\n",
    "1. **Store the Data**: During training, KNN simply stores all the training data points and their labels (no complex model is built).\n",
    "2. **Choose K**: Decide on a number `K`, which is how many neighbors to consider when making a prediction. Common choices are 3, 5, or 7 (odd numbers help avoid ties).\n",
    "3. **Measure Distance**: For a new data point, calculate the \"distance\" to all points in the training data to find the closest ones. Distance is often measured using Euclidean distance (like a straight-line distance on a graph between two points).\n",
    "4. **Find K Nearest Neighbors**: Identify the `K` training points closest to the new point.\n",
    "5. **Make a Prediction**:\n",
    "   - For **classification**: Take a majority vote among the `K` neighbors’ labels (e.g., if 3 out of 5 neighbors are \"spam,\" predict \"spam\").\n",
    "   - For **regression**: Take the average of the `K` neighbors’ values (e.g., average their house prices).\n",
    "\n",
    "**Analogy**: Think of KNN as asking for advice from your closest friends. If you’re deciding what restaurant to try, you ask your 3 nearest friends (by location or taste) for their favorite nearby spot. If most say \"Pizza Place,\" you go there. KNN predicts by consulting the nearest data points in a similar way.\n",
    "\n",
    "**Key Parameter**: The choice of `K` matters. A small `K` (like 1) can be too sensitive to noise (overfitting), while a large `K` might smooth over important differences (underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Classifying Flowers with KNN\n",
    "\n",
    "Let’s see KNN in action with a simplified dataset inspired by the famous Iris dataset. We’ll classify flowers into two species (Setosa or Versicolor) based on two features: petal length and petal width.\n",
    "\n",
    "**Dataset** (simplified):\n",
    "- Petal Length (cm): 1.4, 1.5, 4.5, 4.0, 1.3\n",
    "- Petal Width (cm): 0.2, 0.2, 1.5, 1.3, 0.3\n",
    "- Species (Label): Setosa, Setosa, Versicolor, Versicolor, Setosa\n",
    "\n",
    "We’ll use Python’s `scikit-learn` library to create a KNN model, train it on this data, and predict the species of a new flower. Focus on the steps and output, not the code details.\n",
    "\n",
    "**Instructions**: Run the code below to see how KNN classifies flowers and visualizes the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Our small dataset\n",
    "X = np.array([[1.4, 0.2], [1.5, 0.2], [4.5, 1.5], [4.0, 1.3], [1.3, 0.3]])  # Features: petal length, petal width\n",
    "y = np.array(['Setosa', 'Setosa', 'Versicolor', 'Versicolor', 'Setosa'])  # Labels: species\n",
    "\n",
    "# Create and train the KNN model with K=3\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict for a new flower with petal length=2.0 cm, petal width=0.5 cm\n",
    "new_flower = np.array([[2.0, 0.5]])\n",
    "prediction = model.predict(new_flower)[0]\n",
    "print(f\"New Flower (petal length=2.0 cm, petal width=0.5 cm): Predicted as {prediction}\")\n",
    "\n",
    "# Get the indices of the 3 nearest neighbors\n",
    "distances, indices = model.kneighbors(new_flower)\n",
    "print(f\"Distances to 3 nearest neighbors: {distances[0]}\")\n",
    "print(f\"Indices of 3 nearest neighbors: {indices[0]}\")\n",
    "print(f\"Species of 3 nearest neighbors: {y[indices[0]]}\")\n",
    "\n",
    "# Visualize the data and nearest neighbors\n",
    "# Create a mesh grid for decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.2, X[:, 1].max() + 0.2\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.05))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.array([1 if z == 'Versicolor' else 0 for z in Z]).reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and points\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X[y == 'Setosa'][:, 0], X[y == 'Setosa'][:, 1], color='blue', label='Setosa', alpha=0.8)\n",
    "plt.scatter(X[y == 'Versicolor'][:, 0], X[y == 'Versicolor'][:, 1], color='red', label='Versicolor', alpha=0.8)\n",
    "plt.scatter(new_flower[0][0], new_flower[0][1], color='green', marker='x', s=200, label='New Flower')\n",
    "# Highlight the nearest neighbors\n",
    "for idx in indices[0]:\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], color='yellow', edgecolor='black', s=150, alpha=0.5, label='Neighbor' if idx == indices[0][0] else '')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title('K-Nearest Neighbors (K=3): Flower Classification')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot above:\")\n",
    "print(\"- Blue dots are Setosa flowers.\")\n",
    "print(\"- Red dots are Versicolor flowers.\")\n",
    "print(\"- The colored background shows the decision regions based on K=3 nearest neighbors.\")\n",
    "print(\"- The green 'X' is the new flower being classified.\")\n",
    "print(\"- Yellow highlighted points are the 3 nearest neighbors used for the prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Exercise: Adjust K and Predict\n",
    "\n",
    "Now it’s your turn to experiment with KNN! In this exercise, you can adjust the number of neighbors (`K`) and add a new flower to the dataset to see how the prediction changes. You’ll also choose a new flower to classify.\n",
    "\n",
    "**Instructions**:\n",
    "- Run the code below.\n",
    "- Enter a value for `K` (number of neighbors, e.g., 1, 3, 5).\n",
    "- Add a new flower by specifying petal length, petal width, and species.\n",
    "- Specify a flower to predict by entering its petal length and width.\n",
    "- Observe how the prediction and nearest neighbors change with different `K` values and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exercise for KNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "print(\"Welcome to the 'Adjust K and Predict' Exercise!\")\n",
    "print(\"You’ll adjust the number of neighbors (K), add a new flower, and predict the species of another flower.\")\n",
    "\n",
    "# Original dataset\n",
    "X = np.array([[1.4, 0.2], [1.5, 0.2], [4.5, 1.5], [4.0, 1.3], [1.3, 0.3]])\n",
    "y = np.array(['Setosa', 'Setosa', 'Versicolor', 'Versicolor', 'Setosa'])\n",
    "\n",
    "# Ask user for K value\n",
    "try:\n",
    "    k = int(input(\"Enter the number of neighbors (K, e.g., 3): \"))\n",
    "    if k < 1 or k > len(X):\n",
    "        raise ValueError(f\"K must be between 1 and {len(X)}.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Invalid input: {e}. Defaulting to K=3.\")\n",
    "    k = 3\n",
    "\n",
    "# Ask user to add a new data point\n",
    "try:\n",
    "    new_length = float(input(\"Enter petal length for new flower (cm, e.g., 2.5): \"))\n",
    "    new_width = float(input(\"Enter petal width for new flower (cm, e.g., 0.5): \"))\n",
    "    new_species = input(\"Enter species for new flower (Setosa/Versicolor): \").strip().capitalize()\n",
    "    if new_species not in ['Setosa', 'Versicolor']:\n",
    "        raise ValueError(\"Species must be Setosa or Versicolor.\")\n",
    "    X = np.vstack([X, [new_length, new_width]])\n",
    "    y = np.append(y, new_species)\n",
    "    print(f\"Added flower: petal length={new_length} cm, petal width={new_width} cm, species={new_species}.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Invalid input: {e}. Using original data without changes.\")\n",
    "\n",
    "# Train the model with updated data\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Ask user for a new flower to predict\n",
    "try:\n",
    "    predict_length = float(input(\"Enter petal length to predict species (cm, e.g., 2.0): \"))\n",
    "    predict_width = float(input(\"Enter petal width to predict species (cm, e.g., 0.5): \"))\n",
    "    new_flower = np.array([[predict_length, predict_width]])\n",
    "    prediction = model.predict(new_flower)[0]\n",
    "    print(f\"Predicted species for flower (petal length={predict_length} cm, petal width={predict_width} cm): {prediction}\")\n",
    "    distances, indices = model.kneighbors(new_flower)\n",
    "    print(f\"Distances to {k} nearest neighbors: {distances[0]}\")\n",
    "    print(f\"Species of {k} nearest neighbors: {y[indices[0]]}\")\n",
    "except ValueError:\n",
    "    new_flower = np.array([[2.0, 0.5]])\n",
    "    prediction = model.predict(new_flower)[0]\n",
    "    print(f\"Invalid input. Defaulting to petal length=2.0 cm, petal width=0.5 cm. Predicted species: {prediction}\")\n",
    "    distances, indices = model.kneighbors(new_flower)\n",
    "    print(f\"Distances to {k} nearest neighbors: {distances[0]}\")\n",
    "    print(f\"Species of {k} nearest neighbors: {y[indices[0]]}\")\n",
    "\n",
    "# Visualize the updated data and nearest neighbors\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.2, X[:, 1].max() + 0.2\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.05))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.array([1 if z == 'Versicolor' else 0 for z in Z]).reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X[:-1][y[:-1] == 'Setosa'][:, 0], X[:-1][y[:-1] == 'Setosa'][:, 1], color='blue', label='Original Setosa', alpha=0.8)\n",
    "plt.scatter(X[:-1][y[:-1] == 'Versicolor'][:, 0], X[:-1][y[:-1] == 'Versicolor'][:, 1], color='red', label='Original Versicolor', alpha=0.8)\n",
    "plt.scatter(X[-1, 0], X[-1, 1], color='orange', label=f\"Your Added Data ({y[-1]})\", alpha=0.8)\n",
    "plt.scatter(new_flower[0][0], new_flower[0][1], color='green', marker='x', s=200, label='Prediction')\n",
    "# Highlight the nearest neighbors\n",
    "for idx in indices[0]:\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], color='yellow', edgecolor='black', s=150, alpha=0.5, label='Neighbor' if idx == indices[0][0] else '')\n",
    "plt.xlabel('Petal Length (cm)')\n",
    "plt.ylabel('Petal Width (cm)')\n",
    "plt.title(f'K-Nearest Neighbors (K={k}): Flower Classification')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Look at the plot above:\")\n",
    "print(\"- Blue dots are original Setosa flowers.\")\n",
    "print(\"- Red dots are original Versicolor flowers.\")\n",
    "print(\"- Orange dot is the flower data you added.\")\n",
    "print(\"- The colored background shows the decision regions based on K nearest neighbors.\")\n",
    "print(\"- The green 'X' is the flower being classified.\")\n",
    "print(f\"- Yellow highlighted points are the {k} nearest neighbors used for the prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Considerations for K-Nearest Neighbors\n",
    "\n",
    "KNN is simple and intuitive, but it comes with some considerations to keep in mind:\n",
    "\n",
    "- **Choosing K**: The value of `K` significantly affects predictions. A small `K` can be noisy and overfit (too sensitive to individual points), while a large `K` can underfit (oversmoothing by considering too many distant points).\n",
    "- **Computationally Expensive**: Since KNN stores all training data and calculates distances for every prediction, it can be slow for large datasets. It’s not ideal for big data.\n",
    "- **Sensitive to Feature Scaling**: If features are on different scales (e.g., one feature in meters, another in centimeters), distance calculations can be skewed. Features often need to be normalized or standardized.\n",
    "- **Curse of Dimensionality**: KNN struggles in high-dimensional spaces (many features) because distances become less meaningful as dimensions increase.\n",
    "\n",
    "**Analogy**: KNN is like asking for opinions from nearby friends. If you ask too few (small K), one odd opinion can mislead you. If you ask too many (large K), you might dilute good advice. Also, if your friends are far away or speak different \"languages\" (unscaled features), their advice might not make sense.\n",
    "\n",
    "Despite these limitations, KNN is a great starting point for understanding similarity-based learning and can be effective for small, well-scaled datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "- **K-Nearest Neighbors (KNN)** is a supervised learning algorithm for classification and regression, predicting based on the similarity (distance) to nearby data points.\n",
    "- It works by finding the `K` closest training points to a new point and using majority vote (classification) or average (regression) for prediction.\n",
    "- Use it for tasks like species classification or price prediction when similarity matters and datasets are small to medium-sized.\n",
    "- Be aware of limitations: choosing the right `K` is crucial, it’s slow for large data, and it requires scaled features and low-dimensional data for best results.\n",
    "\n",
    "You’ve now learned a fundamental similarity-based algorithm! KNN introduces the concept of distance and neighbor-based learning, which is a building block for other methods in machine learning.\n",
    "\n",
    "**What's Next?**\n",
    "Move on to **Notebook 8: Support Vector Machines** to learn about a powerful algorithm for classification that finds the best boundary to separate classes. See you there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}