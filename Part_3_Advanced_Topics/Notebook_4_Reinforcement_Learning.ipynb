{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Reinforcement Learning\n",
    "\n",
    "Welcome to the fourth notebook in our advanced machine learning series under **Part_3_Advanced_Topics**. In this notebook, we will explore **Reinforcement Learning (RL)**, a paradigm of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What is Reinforcement Learning?\n",
    "- Key concepts: Agent, Environment, Reward, and Policy\n",
    "- How Reinforcement Learning works\n",
    "- Implementation of a basic Q-Learning algorithm\n",
    "- Advantages and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning is a type of machine learning where an agent learns optimal behavior through trial and error by interacting with an environment. Unlike supervised learning, where the model is trained on labeled data, or unsupervised learning, where patterns are found in unlabeled data, RL focuses on learning a strategy (policy) to maximize a long-term reward.\n",
    "\n",
    "RL is widely used in robotics, game playing (e.g., AlphaGo), recommendation systems, and autonomous systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Agent:** The learner or decision-maker that interacts with the environment (e.g., a robot or game player).\n",
    "- **Environment:** The external system with which the agent interacts, providing states, rewards, and responses to actions (e.g., a game board or physical space).\n",
    "- **State (S):** A representation of the current situation or configuration of the environment at a given time.\n",
    "- **Action (A):** A decision or move made by the agent that affects the environment.\n",
    "- **Reward (R):** A numerical value given by the environment to the agent as feedback on the desirability of an action in a given state.\n",
    "- **Policy (Ï€):** A strategy or mapping from states to actions that the agent uses to decide what to do. The goal is to learn an optimal policy.\n",
    "- **Value Function (V or Q):** Estimates the expected future reward for being in a state (V) or taking an action in a state (Q), guiding the agent's decisions.\n",
    "- **Exploration vs. Exploitation:** The trade-off between trying new actions to discover better rewards (exploration) and sticking to known rewarding actions (exploitation).\n",
    "- **Q-Learning:** A model-free RL algorithm that learns an action-value function (Q-table) to determine the best action for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Reinforcement Learning Works\n",
    "\n",
    "Reinforcement Learning operates on the following iterative process, often modeled as a Markov Decision Process (MDP):\n",
    "\n",
    "1. **Initialization:** Start with an initial state and a policy (often random) or value function.\n",
    "2. **Interaction:** The agent observes the current state of the environment.\n",
    "3. **Action Selection:** Based on the policy, the agent chooses an action (balancing exploration and exploitation).\n",
    "4. **Environment Response:** The environment transitions to a new state and provides a reward based on the action taken.\n",
    "5. **Learning Update:** The agent updates its knowledge (e.g., Q-values) using the reward and new state information to improve future decisions.\n",
    "6. **Repeat:** Continue steps 2-5 for many episodes or until a termination condition is met (e.g., reaching a goal state or maximum iterations).\n",
    "7. **Convergence:** Over time, the agent learns an optimal policy that maximizes cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a Basic Q-Learning Algorithm\n",
    "\n",
    "Let's implement a simple Q-Learning algorithm to solve a grid world problem. In this environment, the agent must navigate from a starting point to a goal while avoiding obstacles, learning through rewards and penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the grid world environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.grid = np.zeros((size, size))\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.obstacles = [(1, 1), (2, 2), (3, 1)]  # Positions to avoid\n",
    "        self.state = self.start\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Right, Down, Left, Up\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state = (self.state[0] + action[0], self.state[1] + action[1])\n",
    "        # Check boundaries\n",
    "        if (0 <= next_state[0] < self.size) and (0 <= next_state[1] < self.size):\n",
    "            if next_state in self.obstacles:\n",
    "                reward = -10  # Penalty for hitting an obstacle\n",
    "                done = False\n",
    "            else:\n",
    "                self.state = next_state\n",
    "                if self.state == self.goal:\n",
    "                    reward = 100  # Reward for reaching the goal\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = -1  # Small penalty for each step to encourage efficiency\n",
    "                    done = False\n",
    "        else:\n",
    "            reward = -5  # Penalty for hitting a wall\n",
    "            done = False\n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs] = -1  # Obstacle\n",
    "        grid[self.goal] = 2  # Goal\n",
    "        grid[self.state] = 1  # Agent position\n",
    "        return grid\n",
    "\n",
    "# Q-Learning Algorithm\n",
    "def q_learning(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    n_actions = len(env.actions)\n",
    "    q_table = np.zeros((env.size, env.size, n_actions))  # State-action value table\n",
    "    rewards_per_episode = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy policy for exploration vs exploitation\n",
    "            if np.random.random() < epsilon:\n",
    "                action_idx = np.random.randint(n_actions)  # Explore\n",
    "            else:\n",
    "                action_idx = np.argmax(q_table[state[0], state[1]])  # Exploit\n",
    "            \n",
    "            action = env.actions[action_idx]\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-Learning update rule: Q(s,a) <- Q(s,a) + alpha * [reward + gamma * max(Q(s',a')) - Q(s,a)]\n",
    "            old_value = q_table[state[0], state[1], action_idx]\n",
    "            next_max = np.max(q_table[next_state[0], next_state[1]])\n",
    "            new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "            q_table[state[0], state[1], action_idx] = new_value\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        rewards_per_episode.append(total_reward)\n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        epsilon = max(0.01, epsilon * 0.995)\n",
    "    \n",
    "    return q_table, rewards_per_episode\n",
    "\n",
    "# Visualize the learned policy\n",
    "def visualize_policy(env, q_table):\n",
    "    policy = np.argmax(q_table, axis=2)\n",
    "    action_names = ['Right', 'Down', 'Left', 'Up']\n",
    "    grid = env.render()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid, cmap='coolwarm', interpolation='nearest')\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) == env.goal:\n",
    "                plt.text(j, i, 'Goal', ha='center', va='center', color='white')\n",
    "            elif (i, j) in env.obstacles:\n",
    "                plt.text(j, i, 'Obs', ha='center', va='center', color='white')\n",
    "            elif (i, j) == env.start:\n",
    "                plt.text(j, i, 'Start', ha='center', va='center', color='black')\n",
    "            else:\n",
    "                action_idx = policy[i, j]\n",
    "                plt.text(j, i, action_names[action_idx], ha='center', va='center', color='black')\n",
    "    plt.title('Learned Policy in Grid World')\n",
    "    plt.colorbar(label='Grid Values')\n",
    "    plt.show()\n",
    "\n",
    "# Run the Q-Learning experiment\n",
    "env = GridWorld(size=5)\n",
    "q_table, rewards = q_learning(env, episodes=1000)\n",
    "\n",
    "# Plot the learning curve (rewards over episodes)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards, label='Total Reward per Episode')\n",
    "plt.title('Learning Curve: Total Reward Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the learned policy\n",
    "visualize_policy(env, q_table)\n",
    "\n",
    "# Demonstrate the agent following the learned policy\n",
    "print('Demonstrating the learned policy:')\n",
    "state = env.reset()\n",
    "done = False\n",
    "path = [state]\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action_idx = np.argmax(q_table[state[0], state[1]])\n",
    "    action = env.actions[action_idx]\n",
    "    state, reward, done = env.step(action)\n",
    "    path.append(state)\n",
    "    total_reward += reward\n",
    "    print(f'Step: {state}, Reward: {reward}')\n",
    "\n",
    "print(f'Total Reward for the path: {total_reward}')\n",
    "print(f'Path taken: {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Can solve complex decision-making problems where the optimal strategy is not known in advance.\n",
    "- Adapts to dynamic environments through continuous learning from interactions.\n",
    "- Achieves impressive results in areas like game playing and robotics (e.g., AlphaGo, self-driving cars).\n",
    "\n",
    "**Limitations:**\n",
    "- Requires a large number of interactions with the environment to learn, making it computationally expensive and slow for real-world applications.\n",
    "- Sensitive to the design of the reward function; poor design can lead to unintended behaviors.\n",
    "- Exploration-exploitation trade-off can be challenging to balance, potentially leading to suboptimal policies.\n",
    "- Struggles with high-dimensional state or action spaces without advanced techniques like deep reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Reinforcement Learning offers a powerful framework for learning optimal decision-making strategies through interaction with an environment. While basic algorithms like Q-Learning are effective for small, discrete problems like grid worlds, scaling RL to complex tasks often requires advanced methods such as Deep Q-Networks (DQN) or policy gradient methods. Understanding the core concepts of agents, rewards, and policies lays the foundation for tackling real-world challenges with RL.\n",
    "\n",
    "This concludes our series on advanced machine learning topics. We hope these notebooks have expanded your toolkit and provided practical insights into specialized areas of machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}