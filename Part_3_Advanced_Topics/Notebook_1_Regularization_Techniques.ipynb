{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Regularization Techniques (Lasso, Ridge, Elastic Net)\n",
    "\n",
    "Welcome to the first notebook in our advanced machine learning series under **Part_3_Advanced_Topics**. In this notebook, we will explore **Regularization Techniques**, which are essential for preventing overfitting in regression models by adding a penalty to the complexity of the model.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What is Regularization?\n",
    "- Key concepts: Overfitting, L1 (Lasso), L2 (Ridge), and Elastic Net\n",
    "- How Regularization works\n",
    "- Implementation using scikit-learn\n",
    "- Advantages and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Regularization?\n",
    "\n",
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function of a model. Overfitting occurs when a model learns the noise in the training data rather than the underlying pattern, leading to poor generalization on unseen data.\n",
    "\n",
    "Regularization methods like Lasso, Ridge, and Elastic Net constrain the model's complexity by penalizing large coefficients in regression models, thus improving performance on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Overfitting:** When a model is too complex and fits the training data too closely, including noise, resulting in poor performance on new data.\n",
    "- **L1 Regularization (Lasso):** Adds the absolute values of the coefficients as a penalty term to the loss function. It can drive some coefficients to exactly zero, effectively performing feature selection.\n",
    "- **L2 Regularization (Ridge):** Adds the squared values of the coefficients as a penalty term. It shrinks coefficients towards zero but rarely to exactly zero, helping to reduce model sensitivity to individual features.\n",
    "- **Elastic Net:** Combines L1 and L2 regularization, balancing feature selection (from Lasso) and coefficient shrinkage (from Ridge).\n",
    "- **Hyperparameter (Alpha/Lambda):** Controls the strength of the regularization penalty. A higher value means more regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Regularization Works\n",
    "\n",
    "Regularization modifies the standard loss function (e.g., Mean Squared Error for regression) by adding a penalty term:\n",
    "\n",
    "- **Standard Loss Function:** Minimize the error between predicted and actual values.\n",
    "- **Regularized Loss Function:** Minimize the error + penalty term.\n",
    "  - Lasso: Error + α * Σ|coefficients|\n",
    "  - Ridge: Error + α * Σ(coefficients²)\n",
    "  - Elastic Net: Error + α * [(1 - l1_ratio) * Σ(coefficients²) + l1_ratio * Σ|coefficients|]\n",
    "\n",
    "The penalty term discourages large coefficients, which often indicate overfitting, thus simplifying the model and improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using scikit-learn\n",
    "\n",
    "Let's implement Linear Regression with and without regularization using scikit-learn. We'll compare the performance and observe the effect on model coefficients using a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a synthetic dataset for regression with noise\n",
    "X, y = make_regression(n_samples=100, n_features=10, n_informative=5, noise=15, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Standard Linear Regression (No Regularization)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "print(f'Standard Linear Regression - MSE: {mse_lr:.2f}, R2 Score: {r2_lr:.2f}')\n",
    "print(f'Coefficients (Linear Regression): {lr_model.coef_}')\n",
    "\n",
    "# 2. Lasso Regression (L1 Regularization)\n",
    "lasso_model = Lasso(alpha=1.0, random_state=42)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(f'Lasso Regression - MSE: {mse_lasso:.2f}, R2 Score: {r2_lasso:.2f}')\n",
    "print(f'Coefficients (Lasso): {lasso_model.coef_}')\n",
    "\n",
    "# 3. Ridge Regression (L2 Regularization)\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(f'Ridge Regression - MSE: {mse_ridge:.2f}, R2 Score: {r2_ridge:.2f}')\n",
    "print(f'Coefficients (Ridge): {ridge_model.coef_}')\n",
    "\n",
    "# 4. Elastic Net (Combination of L1 and L2)\n",
    "elastic_model = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)\n",
    "elastic_model.fit(X_train, y_train)\n",
    "y_pred_elastic = elastic_model.predict(X_test)\n",
    "mse_elastic = mean_squared_error(y_test, y_pred_elastic)\n",
    "r2_elastic = r2_score(y_test, y_pred_elastic)\n",
    "print(f'Elastic Net Regression - MSE: {mse_elastic:.2f}, R2 Score: {r2_elastic:.2f}')\n",
    "print(f'Coefficients (Elastic Net): {elastic_model.coef_}')\n",
    "\n",
    "# Visualize the coefficients for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(lr_model.coef_))\n",
    "plt.plot(x, lr_model.coef_, marker='o', label='Linear Regression')\n",
    "plt.plot(x, lasso_model.coef_, marker='s', label='Lasso')\n",
    "plt.plot(x, ridge_model.coef_, marker='^', label='Ridge')\n",
    "plt.plot(x, elastic_model.coef_, marker='d', label='Elastic Net')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Comparison of Coefficients with Different Regularization Techniques')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Prevents overfitting by penalizing large coefficients, leading to better generalization on unseen data.\n",
    "- Lasso can perform feature selection by setting some coefficients to zero, simplifying the model.\n",
    "- Ridge handles multicollinearity well by shrinking correlated feature coefficients.\n",
    "- Elastic Net combines the benefits of both Lasso and Ridge, offering a balance between feature selection and coefficient shrinkage.\n",
    "\n",
    "**Limitations:**\n",
    "- Requires tuning of hyperparameters (alpha for penalty strength, l1_ratio for Elastic Net) to achieve optimal performance.\n",
    "- May not be effective if the model is underfitting or if overfitting is not the primary issue.\n",
    "- Lasso may arbitrarily select one feature among highly correlated ones, which can be undesirable for interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Regularization techniques like Lasso, Ridge, and Elastic Net are crucial tools in machine learning for managing model complexity and preventing overfitting. By understanding the differences between L1 and L2 penalties, you can choose the appropriate method based on your dataset and problem, whether you need feature selection, handling multicollinearity, or a balance of both.\n",
    "\n",
    "In the next notebook, we will explore another advanced topic to further enhance our machine learning toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}