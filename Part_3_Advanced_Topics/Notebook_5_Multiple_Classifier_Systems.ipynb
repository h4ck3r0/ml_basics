{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Multiple Classifier Systems\n",
    "\n",
    "Welcome to the fifth notebook in our advanced machine learning series under **Part_3_Advanced_Topics**. In this notebook, we will explore **Multiple Classifier Systems (MCS)**, a category of ensemble methods that combine predictions from multiple classifiers to achieve better performance than any single classifier alone.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- What are Multiple Classifier Systems?\n",
    "- Key concepts: Diversity, Voting, and Stacking\n",
    "- How Multiple Classifier Systems work\n",
    "- Implementation using scikit-learn\n",
    "- Advantages and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Multiple Classifier Systems?\n",
    "\n",
    "Multiple Classifier Systems, also known as ensemble classifiers, involve combining the predictions of several individual classifiers to produce a final prediction that is often more accurate and robust. The idea is to leverage the strengths of different models to compensate for their individual weaknesses.\n",
    "\n",
    "This approach builds on the concept of ensemble learning, seen in methods like Random Forest and Gradient Boosting, but focuses on combining diverse classifiers (e.g., Decision Trees, SVMs, Logistic Regression) using strategies like voting or stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "- **Diversity:** The effectiveness of an ensemble often depends on the diversity of the base classifiers. Diverse models make different errors, and combining them can reduce overall error.\n",
    "- **Voting Classifier:** A simple ensemble method where each classifier votes on the class label. It can be:\n",
    "  - **Hard Voting:** Each classifier predicts a class label, and the final prediction is the majority vote.\n",
    "  - **Soft Voting:** Each classifier provides a probability for each class, and the final prediction is based on the averaged probabilities.\n",
    "- **Stacking (Stacked Generalization):** A more advanced method where base classifiers make predictions, and a meta-classifier (or meta-learner) is trained on these predictions to make the final decision.\n",
    "- **Base Classifiers:** The individual models (e.g., Logistic Regression, Decision Tree, SVM) whose predictions are combined.\n",
    "- **Error Reduction:** The goal of MCS is to reduce bias (by using complex models) and variance (by averaging predictions), leading to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Multiple Classifier Systems Work\n",
    "\n",
    "Multiple Classifier Systems typically follow these steps:\n",
    "\n",
    "1. **Selection of Base Classifiers:** Choose a set of diverse classifiers that are likely to make different types of errors. Diversity can come from different algorithms, hyperparameters, or training data subsets.\n",
    "2. **Training Base Classifiers:** Train each classifier independently on the training data.\n",
    "3. **Prediction Generation:** Each classifier makes predictions on the test data (or a validation set for stacking).\n",
    "4. **Combination Strategy:**\n",
    "   - **Voting:** Aggregate predictions using majority voting (hard) or averaged probabilities (soft).\n",
    "   - **Stacking:** Use the predictions of base classifiers as features to train a meta-classifier, which makes the final prediction.\n",
    "5. **Final Prediction:** Output the combined prediction, which ideally outperforms any single classifier.\n",
    "\n",
    "The success of MCS relies on the principle that errors made by individual classifiers are not strongly correlated, allowing the ensemble to correct individual mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Using scikit-learn\n",
    "\n",
    "Let's implement two types of Multiple Classifier Systems using scikit-learn: a Voting Classifier and a Stacking Classifier. We'll use a synthetic dataset for classification and compare the ensemble performance against individual classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define individual classifiers for comparison\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "svm = SVC(probability=True, random_state=42)  # probability=True for soft voting\n",
    "\n",
    "# Train and evaluate individual classifiers\n",
    "classifiers = [('Logistic Regression', log_reg), ('Decision Tree', dtree), ('SVM', svm)]\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{name} Accuracy: {accuracy:.2f}')\n",
    "    print(f'{name} Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('-' * 50)\n",
    "\n",
    "# 1. Voting Classifier (Hard and Soft Voting)\n",
    "# Hard Voting\n",
    "voting_hard = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "voting_hard.fit(X_train, y_train)\n",
    "y_pred_hard = voting_hard.predict(X_test)\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)\n",
    "print(f'Voting Classifier (Hard) Accuracy: {accuracy_hard:.2f}')\n",
    "print('Voting Classifier (Hard) Classification Report:')\n",
    "print(classification_report(y_test, y_pred_hard))\n",
    "print('-' * 50)\n",
    "\n",
    "# Soft Voting\n",
    "voting_soft = VotingClassifier(estimators=classifiers, voting='soft')\n",
    "voting_soft.fit(X_train, y_train)\n",
    "y_pred_soft = voting_soft.predict(X_test)\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)\n",
    "print(f'Voting Classifier (Soft) Accuracy: {accuracy_soft:.2f}')\n",
    "print('Voting Classifier (Soft) Classification Report:')\n",
    "print(classification_report(y_test, y_pred_soft))\n",
    "print('-' * 50)\n",
    "\n",
    "# 2. Stacking Classifier\n",
    "# Use the same base classifiers, with Logistic Regression as the meta-classifier\n",
    "stacking = StackingClassifier(estimators=classifiers, final_estimator=LogisticRegression(random_state=42))\n",
    "stacking.fit(X_train, y_train)\n",
    "y_pred_stack = stacking.predict(X_test)\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "print(f'Stacking Classifier Accuracy: {accuracy_stack:.2f}')\n",
    "print('Stacking Classifier Classification Report:')\n",
    "print(classification_report(y_test, y_pred_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "- Often achieves higher accuracy than individual classifiers by combining their strengths and mitigating weaknesses.\n",
    "- Robust to overfitting compared to single complex models, especially with diverse base classifiers.\n",
    "- Flexible framework allowing the use of any combination of classifiers and combination strategies (voting, stacking).\n",
    "\n",
    "**Limitations:**\n",
    "- Increased computational complexity due to training and predicting with multiple models.\n",
    "- Effectiveness depends on diversity; if base classifiers make similar errors, the ensemble may not improve performance.\n",
    "- Stacking requires careful design (e.g., avoiding data leakage by using separate validation sets) and hyperparameter tuning for the meta-classifier.\n",
    "- Less interpretable than single models, as the decision-making process involves multiple layers or votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Multiple Classifier Systems provide a powerful approach to improve prediction performance by combining diverse classifiers through methods like Voting and Stacking. By leveraging the principle of ensemble learning, these systems can outperform individual models, especially in complex classification tasks. Understanding how to select diverse base classifiers and apply combination strategies is key to building effective ensembles.\n",
    "\n",
    "In the next notebook, we will explore another advanced topic to further enhance our machine learning toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}