{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "Welcome to this notebook on Natural Language Processing (NLP), part of the 'Part_4_Deep_Learning_and_Specializations' section of our machine learning tutorial series. In this notebook, we'll explore the fundamentals of NLP, a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. We'll cover traditional and modern approaches to text processing and analysis, with practical examples.\n",
    "\n",
    "## What You'll Learn\n",
    "- The basics of NLP and its applications in machine learning.\n",
    "- Traditional text processing techniques like Bag of Words and TF-IDF.\n",
    "- Modern approaches using word embeddings and transformer models.\n",
    "- How to perform text classification for sentiment analysis using scikit-learn.\n",
    "- An introduction to transformer models with Hugging Face's library.\n",
    "\n",
    "Let's dive into the world of Natural Language Processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that deals with the ability of computers to understand, interpret, and generate human language. NLP encompasses a wide range of tasks, including:\n",
    "- **Text Classification**: Categorizing text into predefined categories (e.g., spam detection, sentiment analysis).\n",
    "- **Named Entity Recognition (NER)**: Identifying entities like names, dates, and organizations in text.\n",
    "- **Machine Translation**: Translating text from one language to another (e.g., Google Translate).\n",
    "- **Question Answering**: Building systems that can answer questions posed in natural language.\n",
    "- **Text Generation**: Creating coherent and contextually relevant text (e.g., chatbots, story generation).\n",
    "\n",
    "NLP has evolved significantly with the advent of deep learning, moving from traditional statistical methods to powerful neural network-based models like transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional NLP Techniques\n",
    "\n",
    "Before deep learning, NLP relied on statistical and rule-based methods to process text. Key techniques include:\n",
    "\n",
    "- **Tokenization**: Splitting text into individual words or tokens.\n",
    "- **Bag of Words (BoW)**: Representing text as a collection of word frequencies, ignoring grammar and word order.\n",
    "- **Term Frequency-Inverse Document Frequency (TF-IDF)**: Weighing words based on their importance in a document relative to a corpus, highlighting unique terms.\n",
    "- **N-grams**: Capturing sequences of words to preserve some context (e.g., bigrams for pairs of words).\n",
    "\n",
    "These methods are simple but effective for many tasks, especially when combined with machine learning algorithms like Naive Bayes or Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modern NLP with Deep Learning\n",
    "\n",
    "Deep learning has transformed NLP by enabling models to learn semantic relationships and context from text. Key advancements include:\n",
    "\n",
    "- **Word Embeddings**: Dense vector representations of words that capture semantic meaning (e.g., Word2Vec, GloVe, fastText). Words with similar meanings are closer in vector space.\n",
    "- **Recurrent Neural Networks (RNNs)**: Models like LSTMs and GRUs for sequential data, useful for tasks like language modeling and sentiment analysis.\n",
    "- **Transformers**: A breakthrough architecture introduced in the paper \"Attention is All You Need\" (2017). Transformers use self-attention mechanisms to process entire sequences simultaneously, leading to state-of-the-art performance in tasks like translation and text generation. Models like BERT (Bidirectional Encoder Representations from Transformers) have become foundational in NLP.\n",
    "\n",
    "We'll explore both traditional and modern approaches in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting Up the Environment\n",
    "\n",
    "Let's import the necessary libraries for traditional NLP with scikit-learn and introduce modern NLP with Hugging Face's transformers library. We'll also use pandas for data handling and matplotlib for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For modern NLP with transformers (optional installation check)\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except ImportError:\n",
    "    print(\"Transformers library not installed. Install with: pip install transformers\")\n",
    "    pipeline = None\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Traditional NLP: Text Classification with TF-IDF\n",
    "\n",
    "Let's start with a traditional NLP approach by performing text classification for sentiment analysis. We'll use a simple dataset of movie reviews (positive or negative) and apply TF-IDF to transform the text into numerical features, then train a logistic regression model.\n",
    "\n",
    "For this example, we'll create a small synthetic dataset of reviews. In a real scenario, you would use a larger dataset like the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset of movie reviews\n",
    "data = {\n",
    "    'review': [\n",
    "        'This movie was fantastic and I loved every moment',\n",
    "        'Terrible waste of time, hated the plot',\n",
    "        'Amazing acting and a great story',\n",
    "        'Awful, the worst film I have seen',\n",
    "        'Brilliant direction and stunning visuals',\n",
    "        'Boring and predictable, not worth it',\n",
    "        'A masterpiece of cinema, truly inspiring',\n",
    "        'Disappointing, expected much better',\n",
    "        'Loved the characters and the soundtrack',\n",
    "        'Horrible, could not even finish it'\n",
    "    ],\n",
    "    'sentiment': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataset\n",
    "print(\"Sample dataset:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Feature Extraction with TF-IDF\n",
    "\n",
    "We'll use the TfidfVectorizer to convert text into TF-IDF features, which weigh words based on their frequency in a document and rarity across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Transform text to TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Shape of training data (TF-IDF): {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of test data (TF-IDF): {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Training a Logistic Regression Model\n",
    "\n",
    "Now, let's train a logistic regression model on the TF-IDF features to classify reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Visualizing Important Features\n",
    "\n",
    "Let's visualize the most important words (features) for positive and negative sentiments based on the coefficients of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and coefficients\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create DataFrame of features and their coefficients\n",
    "feature_importance = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n",
    "feature_importance = feature_importance.sort_values(by='coefficient', ascending=False)\n",
    "\n",
    "# Plot top positive and negative features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='coefficient', y='feature', data=feature_importance.head(5), color='green', label='Positive Sentiment')\n",
    "sns.barplot(x='coefficient', y='feature', data=feature_importance.tail(5), color='red', label='Negative Sentiment')\n",
    "plt.title('Top Words for Positive and Negative Sentiment')\n",
    "plt.xlabel('Coefficient (Importance)')\n",
    "plt.ylabel('Word')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modern NLP: Introduction to Transformers\n",
    "\n",
    "Transformers have revolutionized NLP by enabling models to understand context over long sequences of text through self-attention mechanisms. Models like BERT, GPT, and T5 are pre-trained on massive datasets and can be fine-tuned for specific tasks.\n",
    "\n",
    "We'll use the Hugging Face `transformers` library to demonstrate a pre-trained model for sentiment analysis. If the library isn't installed, you'll need to run `pip install transformers`.\n",
    "\n",
    "**Note**: This section requires an internet connection to download the pre-trained model weights the first time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if transformers library is available\n",
    "if pipeline is not None:\n",
    "    # Initialize sentiment analysis pipeline with a pre-trained model\n",
    "    sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "    \n",
    "    # Test the model on sample texts\n",
    "    texts = [\n",
    "        \"I absolutely loved this movie, it was fantastic!\",\n",
    "        \"This film was terrible, I hated every moment.\"\n",
    "    ]\n",
    "    results = sentiment_analyzer(texts)\n",
    "    \n",
    "    # Display results\n",
    "    for text, result in zip(texts, results):\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Sentiment: {result['label']}, Confidence: {result['score']:.4f}\\n\")\n",
    "else:\n",
    "    print(\"Transformers library not available. Skipping this section. Install with: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've explored the fundamentals of Natural Language Processing (NLP), covering both traditional and modern approaches. We performed text classification for sentiment analysis using TF-IDF and logistic regression, a classic method that remains effective for many tasks. We also introduced transformer models, showcasing their power with a pre-trained sentiment analysis model from Hugging Face.\n",
    "\n",
    "### Key Takeaways\n",
    "- Traditional NLP techniques like TF-IDF transform text into numerical features for machine learning models.\n",
    "- Logistic regression can effectively classify text based on extracted features, with interpretable results.\n",
    "- Modern NLP with transformers captures complex contextual relationships in text, achieving state-of-the-art performance on various tasks.\n",
    "\n",
    "Feel free to experiment with different datasets, tweak the models, or explore other NLP tasks like named entity recognition or text generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Further Exploration\n",
    "\n",
    "If you're interested in diving deeper into NLP, consider exploring:\n",
    "- **Word Embeddings**: Train or use pre-trained embeddings like Word2Vec or GloVe for better text representation.\n",
    "- **Sequence Models**: Experiment with RNNs, LSTMs, or GRUs for tasks like language modeling.\n",
    "- **Fine-Tuning Transformers**: Fine-tune a pre-trained model like BERT on a custom dataset using Hugging Face's tools.\n",
    "- **Advanced Tasks**: Try tasks like machine translation, summarization, or question answering with transformer models.\n",
    "\n",
    "Stay tuned for more specialized topics in this 'Part_4_Deep_Learning_and_Specializations' section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}