{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in Machine Learning\n",
    "\n",
    "Welcome to this notebook on Transfer Learning, part of the 'Part_4_Deep_Learning_and_Specializations' section of our machine learning tutorial series. In this notebook, we'll explore the concept of transfer learning, a powerful technique in deep learning that allows us to leverage pre-trained models for new tasks, especially when data is limited.\n",
    "\n",
    "## What You'll Learn\n",
    "- The basics of transfer learning and its importance in deep learning.\n",
    "- Key concepts like feature extraction and fine-tuning.\n",
    "- How to apply transfer learning using a pre-trained convolutional neural network (CNN) for image classification.\n",
    "- Practical implementation on a small dataset using TensorFlow and Keras.\n",
    "\n",
    "Let's dive into the world of Transfer Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Transfer Learning\n",
    "\n",
    "Transfer learning is a machine learning technique where a model trained on one task is reused or adapted for a different but related task. It is particularly useful in deep learning, where training large neural networks from scratch requires significant data and computational resources.\n",
    "\n",
    "Transfer learning is widely used in:\n",
    "- **Image Classification**: Using pre-trained models like VGG, ResNet, or Inception for custom image recognition tasks.\n",
    "- **Natural Language Processing**: Fine-tuning models like BERT for specific text classification or question-answering tasks.\n",
    "- **Medical Imaging**: Adapting pre-trained models to detect diseases in X-rays or MRIs with limited labeled data.\n",
    "\n",
    "The core idea is to leverage knowledge learned from a large, general dataset (e.g., ImageNet for images) and apply it to a smaller, specific dataset, saving time and improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Concepts in Transfer Learning\n",
    "\n",
    "Transfer learning typically involves two main strategies when using pre-trained models:\n",
    "\n",
    "- **Feature Extraction**: Use the pre-trained model as a fixed feature extractor. The early layers of the model (which capture general features like edges or textures in images) are frozen, and only the final layers are replaced and trained on the new dataset to adapt to the specific task.\n",
    "- **Fine-Tuning**: Unfreeze some or all of the earlier layers of the pre-trained model and retrain them along with the new layers on the target dataset. This allows the model to adjust its learned features to be more specific to the new task, often improving performance but requiring more data and careful tuning to avoid overfitting.\n",
    "\n",
    "Key benefits of transfer learning include:\n",
    "- Reduced training time since the model starts with pre-learned weights.\n",
    "- Better performance on small datasets by leveraging general features learned from large datasets.\n",
    "- Lower computational requirements compared to training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Environment\n",
    "\n",
    "Let's import the necessary libraries. We'll use TensorFlow and Keras to load a pre-trained model and adapt it for a new task. We'll also use matplotlib for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading a Pre-Trained Model\n",
    "\n",
    "We'll use the VGG16 model, pre-trained on the ImageNet dataset, which contains over 14 million images across 1,000 classes. We'll load the model without the top (fully connected) layers so we can add our own layers for a custom classification task.\n",
    "\n",
    "**Note**: The first time you run this, it will download the pre-trained weights, requiring an internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 model pre-trained on ImageNet, excluding the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the convolutional base to use it as a feature extractor\n",
    "base_model.trainable = False\n",
    "\n",
    "# Display the base model summary\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a Small Synthetic Dataset\n",
    "\n",
    "For demonstration purposes, we'll simulate a small dataset for a binary classification task (e.g., distinguishing between cats and dogs). In a real scenario, you would use a dataset like the Cats vs. Dogs dataset from Kaggle. Since downloading a real dataset and preprocessing it might be complex here, we'll assume we have a small set of images and focus on the model-building process.\n",
    "\n",
    "We'll use TensorFlow's data augmentation and image loading utilities to prepare data. For simplicity, we'll create placeholder data to illustrate the process. Replace this with actual image data in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for data loading (replace with actual dataset in practice)\n",
    "# Simulating a small dataset of images (224x224x3) for binary classification\n",
    "n_train = 100\n",
    "n_test = 20\n",
    "X_train = np.random.rand(n_train, 224, 224, 3)  # Placeholder for training images\n",
    "y_train = np.random.randint(0, 2, n_train)      # Placeholder for binary labels (0 or 1)\n",
    "X_test = np.random.rand(n_test, 224, 224, 3)    # Placeholder for test images\n",
    "y_test = np.random.randint(0, 2, n_test)        # Placeholder for test labels\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Model with Transfer Learning\n",
    "\n",
    "We'll create a new model by adding custom layers on top of the pre-trained VGG16 base. We'll use the feature extraction approach, where the base model is frozen, and only the new layers are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the transfer learning model\n",
    "model = models.Sequential([\n",
    "    base_model,  # Pre-trained VGG16 base\n",
    "    layers.Flatten(),  # Flatten the output of the base model\n",
    "    layers.Dense(256, activation='relu'),  # Add a dense layer\n",
    "    layers.Dropout(0.5),  # Add dropout to prevent overfitting\n",
    "    layers.Dense(2, activation='softmax')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Model\n",
    "\n",
    "Let's train the model on our small dataset for a few epochs. Since we're using transfer learning, even a small dataset can yield reasonable results because the base model already has learned general features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=5, \n",
    "                    batch_size=32, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating the Model\n",
    "\n",
    "After training, let's evaluate the model's performance on the test dataset. Note that since we're using placeholder data, the results are illustrative. With real image data, you would see meaningful accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Training Progress\n",
    "\n",
    "Let's plot the training and validation accuracy and loss over the epochs to understand how the model learned. Again, with placeholder data, this is for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've explored transfer learning, a technique that leverages pre-trained models to solve new tasks with limited data. We used the VGG16 model pre-trained on ImageNet as a feature extractor and added custom layers for a binary classification task. Although we used placeholder data for demonstration, the process applies to real datasets like image classification problems.\n",
    "\n",
    "### Key Takeaways\n",
    "- Transfer learning allows us to reuse pre-trained models, saving time and improving performance on small datasets.\n",
    "- Feature extraction freezes the base model to use learned features, while fine-tuning adjusts the base model for better task-specific performance.\n",
    "- Pre-trained models like VGG16 are powerful starting points for custom deep learning tasks.\n",
    "\n",
    "Feel free to experiment with real datasets, different pre-trained models, or fine-tuning strategies to deepen your understanding of transfer learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Further Exploration\n",
    "\n",
    "If you're interested in diving deeper into transfer learning, consider exploring:\n",
    "- **Different Pre-Trained Models**: Try models like ResNet, Inception, or EfficientNet for various tasks.\n",
    "- **Fine-Tuning**: Experiment with unfreezing layers of the base model and fine-tuning on your dataset.\n",
    "- **Real Datasets**: Apply transfer learning to datasets like Cats vs. Dogs, CIFAR-10, or custom image collections.\n",
    "- **NLP Transfer Learning**: Use pre-trained models like BERT for text tasks (covered in the NLP notebook).\n",
    "\n",
    "Stay tuned for more specialized topics in this 'Part_4_Deep_Learning_and_Specializations' section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}