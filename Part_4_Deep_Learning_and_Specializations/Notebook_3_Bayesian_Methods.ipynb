{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods in Machine Learning\n",
    "\n",
    "Welcome to this notebook on Bayesian Methods, part of the 'Part_4_Deep_Learning_and_Specializations' section of our machine learning tutorial series. In this notebook, we'll explore the fundamentals of Bayesian approaches to machine learning, focusing on probabilistic models and inference techniques. Bayesian methods are powerful for handling uncertainty and making decisions based on probabilistic reasoning.\n",
    "\n",
    "## What You'll Learn\n",
    "- The basics of Bayesian inference and its role in machine learning.\n",
    "- Key concepts like prior, likelihood, and posterior distributions.\n",
    "- How to apply Bayesian methods to regression problems.\n",
    "- Practical implementation of Bayesian linear regression on a synthetic dataset.\n",
    "\n",
    "Let's dive into the world of Bayesian Methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Bayesian Methods\n",
    "\n",
    "Bayesian methods in machine learning are based on Bayesian probability, a framework for reasoning about uncertainty. Unlike frequentist approaches that treat probabilities as long-run frequencies, Bayesian methods interpret probability as a degree of belief that can be updated with new evidence.\n",
    "\n",
    "Bayesian approaches are used in various machine learning tasks, including:\n",
    "- **Classification**: Models like Naive Bayes for spam detection or text categorization.\n",
    "- **Regression**: Bayesian regression for modeling uncertainty in predictions.\n",
    "- **Bayesian Neural Networks**: Incorporating uncertainty in deep learning models.\n",
    "- **Decision Making**: Optimizing decisions under uncertainty, such as in reinforcement learning.\n",
    "\n",
    "The core idea of Bayesian inference is updating beliefs (prior knowledge) with new data (likelihood) to obtain updated beliefs (posterior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian Inference: Core Concepts\n",
    "\n",
    "Bayesian inference revolves around Bayes' Theorem, which mathematically describes how to update probabilities based on new evidence. The theorem is expressed as:\n",
    "\n",
    "$$ P(\\theta|D) = \\frac{P(D|\\theta) \\cdot P(\\theta)}{P(D)} $$\n",
    "\n",
    "Where:\n",
    "- **$P(\\theta|D)$**: Posterior probability of the parameters $\\theta$ given the data $D$.\n",
    "- **$P(D|\\theta)$**: Likelihood of observing the data $D$ given the parameters $\\theta$.\n",
    "- **$P(\\theta)$**: Prior probability of the parameters $\\theta$, representing our initial beliefs.\n",
    "- **$P(D)$**: Marginal likelihood (or evidence), a normalizing constant often difficult to compute directly.\n",
    "\n",
    "In practice, we often focus on the proportional relationship: Posterior ∝ Likelihood × Prior.\n",
    "\n",
    "Bayesian methods are particularly useful for:\n",
    "- Quantifying uncertainty in model parameters and predictions.\n",
    "- Incorporating prior knowledge into models.\n",
    "- Handling small datasets by leveraging priors to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian vs. Frequentist Approaches\n",
    "\n",
    "A key distinction in statistical modeling is between Bayesian and frequentist approaches:\n",
    "\n",
    "- **Frequentist**: Treats parameters as fixed but unknown, and probability as the long-run frequency of events. Methods like Maximum Likelihood Estimation (MLE) find point estimates of parameters.\n",
    "- **Bayesian**: Treats parameters as random variables with probability distributions, allowing for uncertainty quantification. Instead of point estimates, Bayesian methods provide full posterior distributions over parameters.\n",
    "\n",
    "For example, in frequentist linear regression, we get a single set of coefficients. In Bayesian linear regression, we get a distribution over possible coefficients, enabling us to say, \"There's a 95% chance the true coefficient lies within this range.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting Up the Environment\n",
    "\n",
    "Let's import the necessary libraries. We'll use NumPy for numerical operations, scikit-learn for a simple implementation, and matplotlib for visualizations. For more advanced Bayesian modeling, libraries like PyMC3 or Stan can be used, but we'll keep it simple here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a Synthetic Dataset\n",
    "\n",
    "To demonstrate Bayesian linear regression, we'll create a synthetic dataset with one feature and some noise. This will allow us to compare Bayesian methods with traditional linear regression and visualize uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "n_samples = 100\n",
    "X = np.linspace(-1, 1, n_samples).reshape(-1, 1)\n",
    "true_slope = 2\n",
    "true_intercept = 1\n",
    "noise = np.random.normal(0, 0.5, n_samples)\n",
    "y = true_slope * X.flatten() + true_intercept + noise\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Dataset for Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Linear Regression\n",
    "\n",
    "We'll use scikit-learn's `BayesianRidge` model, which implements Bayesian linear regression. This model assumes Gaussian priors on the weights and estimates both the weights and the precision (inverse variance) of the noise. It provides not just point estimates but also uncertainty in the form of standard deviations for the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bayesian Ridge Regression model\n",
    "bayesian_model = BayesianRidge(compute_score=True)\n",
    "bayesian_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred, y_std = bayesian_model.predict(X_test, return_std=True)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on test set: {mse:.4f}\")\n",
    "print(f\"Estimated coefficients: {bayesian_model.coef_}\")\n",
    "print(f\"Estimated intercept: {bayesian_model.intercept_}\")\n",
    "print(f\"Standard deviation of predictions (first 5): {y_std[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Predictions with Uncertainty\n",
    "\n",
    "One of the strengths of Bayesian methods is the ability to quantify uncertainty. Let's plot the predictions along with confidence intervals derived from the standard deviations of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points for smooth prediction line\n",
    "X_smooth = np.linspace(-1, 1, 200).reshape(-1, 1)\n",
    "y_smooth, y_smooth_std = bayesian_model.predict(X_smooth, return_std=True)\n",
    "\n",
    "# Plot data and predictions with uncertainty\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='red', label='Test Data')\n",
    "plt.plot(X_smooth, y_smooth, color='green', label='Bayesian Regression')\n",
    "plt.fill_between(X_smooth.flatten(), \n",
    "                 y_smooth - 1.96 * y_smooth_std, \n",
    "                 y_smooth + 1.96 * y_smooth_std, \n",
    "                 color='green', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Bayesian Linear Regression with Uncertainty')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've explored Bayesian methods in machine learning, focusing on Bayesian linear regression. Unlike traditional methods that provide point estimates, Bayesian approaches offer full distributions over parameters and predictions, allowing us to quantify uncertainty. We implemented Bayesian regression on a synthetic dataset and visualized the uncertainty in predictions.\n",
    "\n",
    "### Key Takeaways\n",
    "- Bayesian inference updates prior beliefs with data to form posterior distributions using Bayes' Theorem.\n",
    "- Bayesian methods are particularly useful for quantifying uncertainty and incorporating prior knowledge.\n",
    "- Visualization of confidence intervals helps in understanding the range of possible outcomes, which is valuable for decision-making.\n",
    "\n",
    "Feel free to experiment with different priors, more complex models, or real-world datasets to deepen your understanding of Bayesian methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Further Exploration\n",
    "\n",
    "If you're interested in diving deeper into Bayesian methods, consider exploring:\n",
    "- **Probabilistic Programming**: Use libraries like PyMC3 or Stan for more flexible Bayesian modeling.\n",
    "- **Bayesian Classification**: Implement Naive Bayes for text classification tasks.\n",
    "- **Bayesian Optimization**: Apply Bayesian methods for hyperparameter tuning in machine learning models.\n",
    "- **Bayesian Neural Networks**: Explore uncertainty in deep learning with Bayesian approaches.\n",
    "\n",
    "Stay tuned for more specialized topics in this 'Part_4_Deep_Learning_and_Specializations' section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}